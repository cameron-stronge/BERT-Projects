{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing.ipynb","provenance":[],"mount_file_id":"1FJVdOhv6Yr24_bsUkeca6RwFrGOeUg7G","authorship_tag":"ABX9TyP73D8QzIYVWrAv1z0UzJVj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hHB_q3JLkVml","executionInfo":{"status":"ok","timestamp":1629823927015,"user_tz":-60,"elapsed":8084,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"033f05b8-960f-4111-faaf-860e8b8fd52e"},"source":["! pip install transformers\n","! pip install datasets"],"execution_count":429,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Collecting datasets\n","  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n","\u001b[K     |████████████████████████████████| 264 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 50.6 MB/s \n","\u001b[?25hCollecting fsspec>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, fsspec, datasets\n","Successfully installed datasets-1.11.0 fsspec-2021.7.0 xxhash-2.0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JKh2UXfNIkT7","executionInfo":{"status":"ok","timestamp":1629824101921,"user_tz":-60,"elapsed":226,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}}},"source":["import pandas as pd\n","import numpy as np\n","import pdb\n","import os\n","os.chdir('/content/drive/MyDrive/CAMemBERT2')\n","import re\n","import string\n","import copy\n","from transformers import AutoTokenizer\n","from datasets import DatasetDict,Dataset"],"execution_count":434,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"ZgRiGlP6jei6","executionInfo":{"status":"error","timestamp":1629828498591,"user_tz":-60,"elapsed":3931,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"8507d343-ae5a-4ebc-f707-7459aa47266b"},"source":["class PreProcessing:\n","\n","    _dataset = 'fce'\n","    _set_types = ['train','test','dev']\n","    _path_to_dataset = 'data/'\n","    _script_score_mapping = {\n","\n","                      1.1:1,1.2:4,1.3:8,\n","                  \n","                      2.1:9,2.2:10,2.3:11,\n","                  \n","                      3.1:12,3.2:13,3.3:14,\n","                  \n","                      4.1:15,4.2:16,4.3:17,\n","                  \n","                      5.1:18,5.2:19,5.3:20,\n","                  \n","                  }\n","    _no_ws = re.compile( '\\S+' )\n","    _punc_strt_or_end = re.compile( f'(^[{string.punctuation}]+|[{string.punctuation}]+$)' )\n","    _only_punc = re.compile(f'^[{string.punctuation}]+$')\n","\n","    # initialise the class so that all pre-processing steps are completed\n","    def __init__(self,pretrained_model='distilroberta-base',max_length=512):\n","        self.pretrained_model = pretrained_model\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","        self.max_length = max_length\n","        self.data = self.read_data()\n","        self.cleaned_data = self.clean_data(self.data.copy())\n","        self.labeled_data = self.create_labels(self.cleaned_data.copy())\n","        self.data_for_script_scoring = self.prepare_for_script_scoring(self.labeled_data)\n","        self.dataset = self.create_dataset(self.labeled_data.copy())\n","        self.dataset_script = self.create_dataset(self.data_for_script_scoring.copy())\n","\n","    def get_data(self):\n","        return self.data\n","\n","    def get_cleaned_data(self):\n","        return self.cleaned_data\n","\n","    def get_labeled_data(self):\n","        return self.labeled_data\n","\n","    def get_dataset(self):\n","        return self.dataset\n","\n","    def get_script_dataset(self):\n","        return self.dataset_script\n","\n","    # read each dataset into one pandas dataframe\n","    def read_data(self):\n","        return pd.concat(\n","            [self.add_col(pd.read_json(f'{self._path_to_dataset}{self._dataset}.{set_type}.json',lines=True),'set_type',set_type) for set_type in self._set_types]\n","            ,axis=0\n","        )\n","\n","    # create a new column in a dataframe with one constant value \n","    def add_col(self,df,col,val):\n","        df[col] = val\n","        return df\n","\n","    # method to apply all steps that clean the data.\n","    def clean_data(self,data):\n","        data = self.turn_edits_to_one_list_of_lists(data)\n","        data = self.remove_new_line_breaks_from_text(data)\n","        data = self.clean_answer_scores(data)\n","        data = self.normalise_script_scores(data)\n","        return data\n","\n","    def turn_edits_to_one_list_of_lists(self,df):\n","        df[ 'edits' ] = df[ 'edits' ].apply( lambda x : x[ 0 ][ 1 ] )\n","        df[ 'edits' ] = df[ 'edits' ].apply( lambda x : [ a[:2] for a in x ])\n","        df[ 'edits' ] = df[ 'edits' ].apply( lambda x : self.remove_overlapping_groups(x) )\n","        return df\n","\n","    def remove_overlapping_groups(self,x):\n","        if len(x)>0:\n","            x = [list(range(ind[0],ind[1]+1)) for ind in x]\n","            non_overlapping = []\n","            for i,inds in enumerate(x):\n","                list_with_data_removed = x[:i] + x[i+1 :]\n","                if not any(len(set(inds_2)-set(inds))==0 for inds_2 in list_with_data_removed):\n","                    non_overlapping.append([inds[0],inds[-1]])\n","            return non_overlapping\n","        else:\n","            return x\n","\n","    # cleaning text\n","    def remove_new_line_breaks_from_text(self,df):\n","        df[ 'text' ] = df[ 'text' ].str.replace( '\\n',' ' )\n","        return df\n","\n","    # clean script scores by correcting typos '/' should represent '.'\n","    # remove scores that can't be converted to float and are not in the score mapping dictionary.\n","    # these scores have either been tagged with a T or S suggesting that the essay has not adhered to the prompt\n","    # or do not have a clear mapping and thus might negatively impact on results.\n","    # applies score mapping and nomalises scores.\n","    def clean_answer_scores(self,df):\n","        df[ 'answer-s' ] = df[ 'answer-s' ].str.replace( '/','.' ).str.replace('T','')\n","        df = df[ ~pd.to_numeric( df[ 'answer-s' ] , errors='coerce' ).isna() ]\n","        df = df[ df['answer-s'].astype(float).isin(self._script_score_mapping.keys())]\n","        df[ 'answer-s' ] = df[ 'answer-s' ].astype( float ).map( self._script_score_mapping ).apply( lambda x : (x -1) / 19 )\n","        return df.rename(columns={'answer-s':'answer_scores'})\n","\n","    def normalise_script_scores(self,df):\n","        df[ 'script-s' ] = df[ 'script-s' ].astype( int ).apply( lambda x : x / 40 )\n","        return df.rename(columns={'script-s':'script_scores'})\n","\n","\n","    def create_labels(self,df):\n","        df = df.apply(self.create_tags,axis=1)\n","        return df[['cleaned_text','answer_scores','script_scores','labels','set_type','id']]\n","\n","    def create_tags(self,row):\n","        previous_tagged_word = None\n","        if len(row['edits'])>1:\n","            edits = row['edits']\n","            for i,ind in enumerate(edits):\n","                if ind[0]==ind[1]:\n","                    ind[1] += 1\n","                if row['text'][ind[0]:ind[1]]==' ':\n","                    ind[0],ind[1] = self.tag_to_next_word(ind,row['text'])\n","                    if previous_tagged_word==row['text'][ind[0]:ind[1]] and (prev_index == ind[0] or prev_index == ind[1]):\n","                        first_ind,prev_index = ind[0],ind[1]\n","                        continue\n","                    previous_tagged_word = row['text'][ind[0]:ind[1]]\n","                    first_ind,prev_index = ind[0],ind[1]\n","                \n","                if i == 0:\n","                    if ind[0]!=0:\n","                        if ind[0]!=1:\n","                            first_ind,next_ind = 0,ind[0]\n","                        else:\n","                            first_ind,next_ind = 0,2\n","                        updated_indexes = [[first_ind,next_ind-1,0]] + [ind+[1]]\n","                    else:\n","                        updated_indexes = [ind+[1]]\n","                    prev_index = ind[1]\n","                else:\n","                    first_ind = ind[0]\n","                    if first_ind-prev_index>1:\n","                        updated_indexes = updated_indexes + [[prev_index+1,first_ind-1,0]] + [ind+[1]]\n","                    else:\n","                        updated_indexes = updated_indexes + [ind+[1]]\n","                    prev_index = ind[1]\n","                if i==len(edits)-1:\n","                    if prev_index!=len(row['text']):\n","                        updated_indexes = updated_indexes + [[prev_index+1,len(row['text']),0]]\n","\n","            word_and_tags =[(word,ind[-1]) for ind in updated_indexes for word in row['text'][ind[0]:ind[1]].split()]\n","            \n","            tokens , labels =  zip(*word_and_tags)\n","            r_tags , token2word = [] , []\n","            count = 0\n","            for index, token in enumerate( self.tokenizer.tokenize( ' '.join( tokens ) , truncation = True , padding = False , add_special_tokens = False , max_length = self.max_length ) ):\n","\n","                if ( ( ( ( token.startswith( \"Ġ\" ) == False and index != 0 ) or ( token in tokens[ index - count - 1 ].lower() and index - count - 1 >= 0 ) ) and tokenizer.sep_token == '</s>' ) \n","                    or ( ( token.startswith( \"##\" ) or ( token in tokens[index - count - 1].lower() and index - count - 1 >= 0 ) ) and tokenizer.sep_token == '[SEP]' ) ):\n","\n","                    # r_tags.append( 0 )\n","                    \n","                    count += 1\n","\n","                else:\n","\n","                    try:\n","                        r_tags.append(labels[index - count])\n","                    except:\n","                        pdb.set_trace()\n","\n","                token2word.append( index - count )\n","            row['labels'] = np.pad( r_tags , ( 0 , self.max_length - len( r_tags ) ) , 'constant' , constant_values = ( 0 , -100 ) )\n","            row['cleaned_text'] = ' '.join( tokens )\n","            return row\n","        else:\n","            split_text = row['text'].split()\n","            row['labels'] = [0]*len(split_text[:self.max_length]) + [-100]*(self.max_length - len(split_text[:self.max_length]))\n","            row['cleaned_text'] = ' '.join( split_text )\n","            return row\n","\n","    def tag_to_next_word(self,ind,text):\n","        m = re.search('\\S+',text[ind[0]:])\n","        return ind[0]+m.start(),ind[0]+m.end()\n","\n","    def prepare_for_script_scoring(self,df):\n","        df = df.groupby('id').agg({'cleaned_text':list,'script_scores':list,'set_type':list,'labels':list})\n","        df[ 'cleaned_text' ] = df[ 'cleaned_text' ].str.join(' ')\n","        df[ 'script_scores' ] = df[ 'script_scores' ].apply(lambda x : x[0])\n","        df[ 'set_type' ] = df[ 'set_type' ].apply(lambda x : x[0])\n","        df[ 'labels' ] = df[ 'labels' ].apply(lambda x : np.concatenate([np.array(x[0])[np.array(x[0])!=-100],np.array(x[1])])[:self.max_length]  if len(x)==2 else x[0] )\n","        return df\n","\n","\n","    def get_dataset_dict(self,df):\n","        return {set_type:Dataset.from_pandas(df.groupby('set_type').get_group(set_type)) for set_type in self._set_types}\n","\n","    def pre_processing_func(self,example):\n","        return self.tokenizer( example[ 'cleaned_text' ] , truncation=True , padding = 'max_length' , max_length = self.max_length )\n","\n","    def create_dataset(self,df):\n","        dataset = DatasetDict(self.get_dataset_dict(df))\n","        dataset = dataset.map(self.pre_processing_func)\n","        dataset = dataset.remove_columns(['cleaned_text','set_type','id'])\n","        return dataset\n","\n","    def save_dataset(self,scoring_level='answer'):\n","        if scoring_level=='answer':\n","            dataset = self.get_dataset()\n","        elif scoring_level=='script':\n","            dataset = self.get_script_dataset()\n","        else:\n","            print('score level not implemented, select: answer or script')\n","            return\n","        dataset.save_to_disk(f'{self._path_to_dataset}word_level_dataset/{scoring_level}/{self.pretrained_model}')\n","\n","data = PreProcessing(max_length=512)\n","data.save_dataset()\n","data.save_dataset('script')"],"execution_count":461,"outputs":[{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-461-2feb2adf4a3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_to_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{self._path_to_dataset}word_level_dataset/{scoring_level}/{self.pretrained_model}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-461-2feb2adf4a3c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained_model, max_length)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaned_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaned_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_for_script_scoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_script_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabeled_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-461-2feb2adf4a3c>\u001b[0m in \u001b[0;36mclean_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn_edits_to_one_list_of_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_new_line_breaks_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_answer_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalise_script_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-461-2feb2adf4a3c>\u001b[0m in \u001b[0;36mclean_answer_scores\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'answer-s'\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer-s'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_score_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mmin\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'script-s'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mrange\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'script-s'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'answer-s'\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m'answer-s'\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_script_score_mapping\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'min' referenced before assignment"]}]},{"cell_type":"code","metadata":{"id":"U4BfEmnUO78N","executionInfo":{"status":"ok","timestamp":1629827753510,"user_tz":-60,"elapsed":283,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}}},"source":[""],"execution_count":460,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBxRq4WYRffd"},"source":["\n","\n"],"execution_count":null,"outputs":[]}]}