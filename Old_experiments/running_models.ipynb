{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"running_models.ipynb","provenance":[],"mount_file_id":"1DkwIN9p7260G0jzx8BPdYRcfk2MR5d_p","authorship_tag":"ABX9TyMbRnhkuiVRoPAqVfrycBRV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f1e1cb236c5445d0818093a00a4cfbf7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_abf37574657f4268a66f2312bfb9c0b8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_af7a4c38227d4c438f8ca56fedfc1ca1","IPY_MODEL_9bc127ef53ad4ac6a83dffa27917e263","IPY_MODEL_6a8087213b6e4138bf810367fddc2c65"]}},"abf37574657f4268a66f2312bfb9c0b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af7a4c38227d4c438f8ca56fedfc1ca1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_953498b9fcdd452288fdf2e5898d5f9b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29feaa827c6640a0820fe37886b79283"}},"9bc127ef53ad4ac6a83dffa27917e263":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f0bdc1e6850f4b3594582057635a572d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1057,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1057,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ae8cb2534144f608db50fa82c8a8b05"}},"6a8087213b6e4138bf810367fddc2c65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_23066135b94648119b9ca60e1d8be1aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1057/1057 [00:02&lt;00:00, 489.78ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f45f0ebf4ec5483db7449828f9e31939"}},"953498b9fcdd452288fdf2e5898d5f9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"29feaa827c6640a0820fe37886b79283":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0bdc1e6850f4b3594582057635a572d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3ae8cb2534144f608db50fa82c8a8b05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"23066135b94648119b9ca60e1d8be1aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f45f0ebf4ec5483db7449828f9e31939":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7386f610b364b188e4e13d8d9b3b4ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5269ee0710104db7b0a15de0a4c220db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_56481dcc14d64463a11e6b2687bc5fc9","IPY_MODEL_522db33f44bc4d0ba75f71c295e9c25d","IPY_MODEL_86546f608d3c41198e73d7063d887020"]}},"5269ee0710104db7b0a15de0a4c220db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56481dcc14d64463a11e6b2687bc5fc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1d1ab85e2f5d4ffc8a52206cb96554c6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7fcb5a4639364f06a2c78b079ec5f9c2"}},"522db33f44bc4d0ba75f71c295e9c25d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1166029d1bbb4ac293dc003836bda2c1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":97,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":97,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9bd9bfc281df4ffe9f8a3b8aa82b7e43"}},"86546f608d3c41198e73d7063d887020":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_083b3697cc5d46049c307f5ec8a159ac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97/97 [00:00&lt;00:00, 465.04ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5be116338ab44c6b8b4f64e45b7672e9"}},"1d1ab85e2f5d4ffc8a52206cb96554c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7fcb5a4639364f06a2c78b079ec5f9c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1166029d1bbb4ac293dc003836bda2c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9bd9bfc281df4ffe9f8a3b8aa82b7e43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"083b3697cc5d46049c307f5ec8a159ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5be116338ab44c6b8b4f64e45b7672e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"00812e0c345249a7ac3174ffa48e9318":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ded69d9e840d4ee1a16cbccfc2f81d77","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_86430a5eb5a8418fa6e632e1f82eba5f","IPY_MODEL_cb3b010dc01f40a0a109aef05907a893","IPY_MODEL_19251edf6c6241a7a98bff18fa962008"]}},"ded69d9e840d4ee1a16cbccfc2f81d77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86430a5eb5a8418fa6e632e1f82eba5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ebabfffcfe34a7dafe6f1bfa196195b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de6f54b24fb54a71b22bdef6ca0db9ed"}},"cb3b010dc01f40a0a109aef05907a893":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a9db5deb5281493584003619e2480289","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":79,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":79,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f0d73364594c42b088ca630cc43cc574"}},"19251edf6c6241a7a98bff18fa962008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6966e7c2f8d94f3da17e7a977171fbb6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 79/79 [00:00&lt;00:00, 275.64ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4f75cf6318714111899b1b0fe394572e"}},"2ebabfffcfe34a7dafe6f1bfa196195b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"de6f54b24fb54a71b22bdef6ca0db9ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a9db5deb5281493584003619e2480289":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f0d73364594c42b088ca630cc43cc574":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6966e7c2f8d94f3da17e7a977171fbb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4f75cf6318714111899b1b0fe394572e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0584ccd388ea4ea0949b92dbbb8cde98":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f9b6d65f2d0f4929bc156bbc23925bd5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4ead017c97a44e389c43cd79ec4db3e2","IPY_MODEL_24603dce67da4a88a356d70ab3181f32","IPY_MODEL_b403a767985b44539f7557b4e53fba04"]}},"f9b6d65f2d0f4929bc156bbc23925bd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ead017c97a44e389c43cd79ec4db3e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_57ce14013494470ca1350e2f579b6599","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0547d4a4a3ff4d38b31de76a870ad44b"}},"24603dce67da4a88a356d70ab3181f32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4b9ca0d4205146d5bcc35d7911cb5f38","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1057,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1057,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fc45ec00c8e147399c80fb159fedfc79"}},"b403a767985b44539f7557b4e53fba04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bc52fa6b757c4b279327b7ffedc9c598","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1057/1057 [00:01&lt;00:00, 570.48ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f73177b70c34f3c8bc597c7cf15a220"}},"57ce14013494470ca1350e2f579b6599":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0547d4a4a3ff4d38b31de76a870ad44b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4b9ca0d4205146d5bcc35d7911cb5f38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fc45ec00c8e147399c80fb159fedfc79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc52fa6b757c4b279327b7ffedc9c598":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5f73177b70c34f3c8bc597c7cf15a220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5dbe0e1eb79d45d280554a5d11fe1894":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4699120e2da647bc8dd9516df7b8f814","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_87f8165e326740e69ba06dce9ea1c198","IPY_MODEL_37e22b06c9104a169a6709d90144549c","IPY_MODEL_0f93d143c4aa43a1add2e34cf7bab555"]}},"4699120e2da647bc8dd9516df7b8f814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"87f8165e326740e69ba06dce9ea1c198":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_241a936eeacc45fd8859092702b40546","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f7ccebf7a03d4387b54d4535d1875510"}},"37e22b06c9104a169a6709d90144549c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_abe1e73b86714d18b12acacaa36a7d97","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":97,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":97,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_12a38b9bb7d94ea2b8e274f99551f118"}},"0f93d143c4aa43a1add2e34cf7bab555":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6ffe43a99d484adfbc973e8ace61c22d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 97/97 [00:00&lt;00:00, 405.06ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_34a827c9376c4adbacdf50d14560c9cb"}},"241a936eeacc45fd8859092702b40546":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f7ccebf7a03d4387b54d4535d1875510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"abe1e73b86714d18b12acacaa36a7d97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"12a38b9bb7d94ea2b8e274f99551f118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6ffe43a99d484adfbc973e8ace61c22d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"34a827c9376c4adbacdf50d14560c9cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"380f41d4b5fc45669eaa33ee7d7f4de8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d393b31f8f5461ab462ce715dd4c1d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_76963d58ef314adaa45053f6f81fba58","IPY_MODEL_b1996138da594eacb792f5d8235f677d","IPY_MODEL_2413bf0e8f89443cb57e2c4ed765929d"]}},"4d393b31f8f5461ab462ce715dd4c1d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"76963d58ef314adaa45053f6f81fba58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_804397f9856e47a1bd95a6955d086b71","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0159d192204a4df1bfda778e01bccab8"}},"b1996138da594eacb792f5d8235f677d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d19da038749a46c58adfab9bdd84f755","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":79,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":79,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7af48fc401049cc9d1ad0af9cbb6aeb"}},"2413bf0e8f89443cb57e2c4ed765929d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_21053245339c4a6e80faaa14e632d3f3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 79/79 [00:00&lt;00:00, 427.55ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_22d4869562d14d09ac93f04819d8f4ab"}},"804397f9856e47a1bd95a6955d086b71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0159d192204a4df1bfda778e01bccab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d19da038749a46c58adfab9bdd84f755":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d7af48fc401049cc9d1ad0af9cbb6aeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"21053245339c4a6e80faaa14e632d3f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"22d4869562d14d09ac93f04819d8f4ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e9ff8eb8aba43779f23812680ccdc00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_df1a60fbf26f4a42bf1016521f5943fc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_69a571979e524735ba7cc237c20c7d86","IPY_MODEL_3fbf32e309954156966379d7b12b4afe","IPY_MODEL_52a8a7166b1b4ec7b6d65819d3734a8c"]}},"df1a60fbf26f4a42bf1016521f5943fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69a571979e524735ba7cc237c20c7d86":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_75bff487a489477fa382e683badbdc00","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f1f1df376a448ee9c59b0f7cd9a32af"}},"3fbf32e309954156966379d7b12b4afe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b194038990384a73b448eb94944f7fb0","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":14041,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":14041,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6084c6e7931f49448da4ea02350ade24"}},"52a8a7166b1b4ec7b6d65819d3734a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e88166f843e74fde933e8a342cb58d26","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 14041/14041 [00:02&lt;00:00, 5834.61ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_29bda4046c8248f898c247193e228278"}},"75bff487a489477fa382e683badbdc00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8f1f1df376a448ee9c59b0f7cd9a32af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b194038990384a73b448eb94944f7fb0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6084c6e7931f49448da4ea02350ade24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e88166f843e74fde933e8a342cb58d26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"29bda4046c8248f898c247193e228278":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3373e993f873445f884c5d200dab6c60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ad1ebd71d23e41bbb89df69e53b72625","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_df023b473c8b488489a57b60c6f58771","IPY_MODEL_2a32c3e5cfa5467ea726807e55f8768a","IPY_MODEL_6262a5ae17984c278930895557485916"]}},"ad1ebd71d23e41bbb89df69e53b72625":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df023b473c8b488489a57b60c6f58771":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_547e922454664b9a9794b10eb01ec130","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7cf4fefffed74f14a8c31c1b5e3193f4"}},"2a32c3e5cfa5467ea726807e55f8768a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fc405cb0d4334586a4dfa8d5b61ef806","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3250,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3250,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7ac7a92706964798a18e1ddf421ca372"}},"6262a5ae17984c278930895557485916":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ddf1de32afa84df59e5891185aab9ac7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3250/3250 [00:00&lt;00:00, 5424.74ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1ae8af5f82ff4ecdb7151c2b18fb28f1"}},"547e922454664b9a9794b10eb01ec130":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7cf4fefffed74f14a8c31c1b5e3193f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fc405cb0d4334586a4dfa8d5b61ef806":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7ac7a92706964798a18e1ddf421ca372":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ddf1de32afa84df59e5891185aab9ac7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1ae8af5f82ff4ecdb7151c2b18fb28f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a2fa2a983cd940599bc07c11c3f0a1d6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc2ebdcb64c44f2f985614baa3046edf","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_158e75a57a0c43ce8a905b0e0c2b501e","IPY_MODEL_840fbd9c0ab648e192e8e444d67959fb","IPY_MODEL_e7cc7aad7e15451c853e605e3930a40d"]}},"dc2ebdcb64c44f2f985614baa3046edf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"158e75a57a0c43ce8a905b0e0c2b501e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b26221a057364eb5ad8cdebb185d012e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4df1767e61da422eb01edc49a03bda17"}},"840fbd9c0ab648e192e8e444d67959fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ba62297eec7c45b1be8cfaca10f42138","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":3453,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":3453,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c404eae477d443db8efb30ea04eab5a0"}},"e7cc7aad7e15451c853e605e3930a40d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e07fddaef3ca491bbaa3a77688f22c5e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 3453/3453 [00:00&lt;00:00, 6042.35ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_06eaa66cb95546008dba5bac52daf3c3"}},"b26221a057364eb5ad8cdebb185d012e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4df1767e61da422eb01edc49a03bda17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba62297eec7c45b1be8cfaca10f42138":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c404eae477d443db8efb30ea04eab5a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e07fddaef3ca491bbaa3a77688f22c5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"06eaa66cb95546008dba5bac52daf3c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ea63e8357ab9441e9e745f3f70cdeefd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5813b5f27b7c4cc68882a84f945739ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_81c223da20e94f698f1823307e5a52de","IPY_MODEL_6c0aecfccb2a49019cbb6d247a442b5d","IPY_MODEL_035b2523d0644f68935567225b086506"]}},"5813b5f27b7c4cc68882a84f945739ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"81c223da20e94f698f1823307e5a52de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_488e17fe057d490e98f1da0b1017db77","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49e00b3026434d52a9616193238d1ead"}},"6c0aecfccb2a49019cbb6d247a442b5d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e85ce0e01fd46a382fb5510f9408315","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1057,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1057,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e45b67bd2ae242eea21928b72b3664de"}},"035b2523d0644f68935567225b086506":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fa7893f41c924b1183150247a77ceecd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1057/1057 [00:01&lt;00:00, 687.74ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_83c3a4e019c6472ca93b6c10bd574428"}},"488e17fe057d490e98f1da0b1017db77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"49e00b3026434d52a9616193238d1ead":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e85ce0e01fd46a382fb5510f9408315":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e45b67bd2ae242eea21928b72b3664de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fa7893f41c924b1183150247a77ceecd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"83c3a4e019c6472ca93b6c10bd574428":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dad23542c4a24921b4e671daa46b43aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1b489018650d430c82548faf2e0c8803","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_13dd983edcc34812b23e909b89ce0be3","IPY_MODEL_4269328580794a798273da6c5685f044","IPY_MODEL_73fbcd4adf3d47de984ccc10cfa3728e"]}},"1b489018650d430c82548faf2e0c8803":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"13dd983edcc34812b23e909b89ce0be3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_afdf47369e764964be8dd08c8ec6a253","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1340aa8c26d745598d599bb2875c81af"}},"4269328580794a798273da6c5685f044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7dc4ab720b4a448cb82b230f3629ea3c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":266,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":266,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6283f8ea2bce44b8bdd99cb57dd5b2f4"}},"73fbcd4adf3d47de984ccc10cfa3728e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0bab3e2f3de34c818fc5ebce926f6d6d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 266/266 [00:00&lt;00:00, 672.08ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_255d383015474276beed4e3f49c45c39"}},"afdf47369e764964be8dd08c8ec6a253":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1340aa8c26d745598d599bb2875c81af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7dc4ab720b4a448cb82b230f3629ea3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6283f8ea2bce44b8bdd99cb57dd5b2f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0bab3e2f3de34c818fc5ebce926f6d6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"255d383015474276beed4e3f49c45c39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb87018c5f4e43d5b7c436feb368df97":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_61198616a90a48869ef1a34736b3346c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3054bd8bccfc48dbb766b16701489ce5","IPY_MODEL_fe9efb2f4cbe481b8e7c1f0b29dcc618","IPY_MODEL_e360499b81e8410facac12bab61d8908"]}},"61198616a90a48869ef1a34736b3346c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3054bd8bccfc48dbb766b16701489ce5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_923e0cdb856845099de1ac62945a70d0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9753296db7d14f5ca8a96ed7ba41eaa2"}},"fe9efb2f4cbe481b8e7c1f0b29dcc618":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_df7363944069405d92f3dbc2a60c65bc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":243,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":243,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02026c902fb745328c5fbdfd8ef5dbfa"}},"e360499b81e8410facac12bab61d8908":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c95ec28294f4cc5bb6c5e11d9083bf6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 243/243 [00:00&lt;00:00, 682.60ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8705bac0e739489a9e066c58958ea59f"}},"923e0cdb856845099de1ac62945a70d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9753296db7d14f5ca8a96ed7ba41eaa2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df7363944069405d92f3dbc2a60c65bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"02026c902fb745328c5fbdfd8ef5dbfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c95ec28294f4cc5bb6c5e11d9083bf6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8705bac0e739489a9e066c58958ea59f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a802a5463a9f411688b9177d776b2b2c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4729f6a1cd8d449082f9b32e89729bcc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_168dae62475a43d8a14f1a7095410f77","IPY_MODEL_253c0a325e5247118031c2c2ff68c261","IPY_MODEL_3445cea26a56488aab5f555eb702c641"]}},"4729f6a1cd8d449082f9b32e89729bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"168dae62475a43d8a14f1a7095410f77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1a487301fb1b4369ac717ce372524367","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fae6d1237da142cbb9f1bcc3828cfb99"}},"253c0a325e5247118031c2c2ff68c261":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f2a3225f68464e6a81d22f3a790cd012","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1057,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1057,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e0a7f57aae334fc9a5c68fad2026eed9"}},"3445cea26a56488aab5f555eb702c641":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_33986c667b0341eb90711d76f155d570","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1057/1057 [00:01&lt;00:00, 692.71ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0028d3890b8c471f8c50d15fdc179691"}},"1a487301fb1b4369ac717ce372524367":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fae6d1237da142cbb9f1bcc3828cfb99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f2a3225f68464e6a81d22f3a790cd012":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e0a7f57aae334fc9a5c68fad2026eed9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"33986c667b0341eb90711d76f155d570":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0028d3890b8c471f8c50d15fdc179691":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29443d4588ce4b32874e47d151875c48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_29117fe03c8b4545aaf3eba77405e520","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ef0aa77e6615496cad6bf9a4c1f64417","IPY_MODEL_a2178f2938dc4b8d97c08526ac7fef97","IPY_MODEL_bff13f580c6144daa76efd616ae258ab"]}},"29117fe03c8b4545aaf3eba77405e520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef0aa77e6615496cad6bf9a4c1f64417":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_87909fa20eaa4a1f94cc29529175af28","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9db3bdb0f5174c85a427b0b09d1ec5c4"}},"a2178f2938dc4b8d97c08526ac7fef97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0e1c553dd61745a9bf23863db1093c2a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":266,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":266,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f1044d64f484476be6630755626fc5f"}},"bff13f580c6144daa76efd616ae258ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9a336049d7494f279956ad70086cab77","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 266/266 [00:00&lt;00:00, 746.11ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d1ba0a6a69848aa88beec0f88582511"}},"87909fa20eaa4a1f94cc29529175af28":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9db3bdb0f5174c85a427b0b09d1ec5c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e1c553dd61745a9bf23863db1093c2a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7f1044d64f484476be6630755626fc5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a336049d7494f279956ad70086cab77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4d1ba0a6a69848aa88beec0f88582511":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8cdc8faa31f34356bbc5bf47e77d3b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c7e38a6a2f794f7dad0024f76778567c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4d5a4587bea845dba7f1001db16fd1dc","IPY_MODEL_8f96b3de509741afa3fcebd4b3426055","IPY_MODEL_9ed3665cb4df46daa728623a3e1d0b26"]}},"c7e38a6a2f794f7dad0024f76778567c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d5a4587bea845dba7f1001db16fd1dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_568884ad4ea845fbbd07f3d5f3ad6c61","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_338f113de12d441f87ef8ba2f7cc80a7"}},"8f96b3de509741afa3fcebd4b3426055":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aae4ebc6a37d4f62b27e2c5cbf9f7a9c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":243,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":243,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_219ef01dbdfb4b689b04bb6810da77b2"}},"9ed3665cb4df46daa728623a3e1d0b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2ad8d6b02b3e47e58d942311f9563378","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 243/243 [00:00&lt;00:00, 722.36ex/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c05eedcbc524e2c8e246d3c46114b06"}},"568884ad4ea845fbbd07f3d5f3ad6c61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"338f113de12d441f87ef8ba2f7cc80a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aae4ebc6a37d4f62b27e2c5cbf9f7a9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"219ef01dbdfb4b689b04bb6810da77b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ad8d6b02b3e47e58d942311f9563378":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1c05eedcbc524e2c8e246d3c46114b06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cfc0e4bd83c14477be16730aefaae7b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_81f958dd376e48deb692d7a7db6a7446","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_92f5180c1d4f412388fd6516d5eacec5","IPY_MODEL_7fa178775a5b42db87318d435a7861f9","IPY_MODEL_5556be700e15469d8b1cfe548cc69cd9"]}},"81f958dd376e48deb692d7a7db6a7446":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"92f5180c1d4f412388fd6516d5eacec5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_98130681e1bc4868b3200aa144179f59","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd691601f90c48d09f31b4eb34c0d4fc"}},"7fa178775a5b42db87318d435a7861f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e6e2dc53db0b44e5970166eeee300ba9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2f639400ddcf433c85d55177a3b8c564"}},"5556be700e15469d8b1cfe548cc69cd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e086d545b6fa4aa3b9ab36e223b807f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  1.88ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1e360034823c4e07824669296120b0c3"}},"98130681e1bc4868b3200aa144179f59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fd691601f90c48d09f31b4eb34c0d4fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e6e2dc53db0b44e5970166eeee300ba9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2f639400ddcf433c85d55177a3b8c564":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e086d545b6fa4aa3b9ab36e223b807f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1e360034823c4e07824669296120b0c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1dd8378bacba4a01aa2c9eb8ed558b20":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b590ed2573b74752a930b02e341d8a8b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e38832a548884c1eb2d4b216e36fe41f","IPY_MODEL_69a899519b704dd58e1a4fc3616864af","IPY_MODEL_f7db5430ed1140829f29ba9aaf8d650d"]}},"b590ed2573b74752a930b02e341d8a8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e38832a548884c1eb2d4b216e36fe41f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0c26603fd51d4e92bcceb556c111ea16","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_72eb8e63bb2a47f79be4dbdadd26276c"}},"69a899519b704dd58e1a4fc3616864af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d791948f62dd49978f8914866fcb5cbc","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6c79d3b7a9b348f5b634655387014dcb"}},"f7db5430ed1140829f29ba9aaf8d650d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_07d8939e62734b7980cc6c2b29c67257","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  5.77ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e57c6133dca44cd494f42d9949e0e566"}},"0c26603fd51d4e92bcceb556c111ea16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"72eb8e63bb2a47f79be4dbdadd26276c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d791948f62dd49978f8914866fcb5cbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6c79d3b7a9b348f5b634655387014dcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07d8939e62734b7980cc6c2b29c67257":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e57c6133dca44cd494f42d9949e0e566":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83a849b93f8f4ccfae91349fda05485b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_145029449e2743089e0357e31412e019","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_467895bbd1784db5bdb8cb2bc7589053","IPY_MODEL_385d77d6d3f3433dba7abee844aa9a10","IPY_MODEL_36f815ff8e6d45f488f5c4734c039f6a"]}},"145029449e2743089e0357e31412e019":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"467895bbd1784db5bdb8cb2bc7589053":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1eb4e46b44a2461eb06781f34d799996","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ce166393d8ff47ac894262b7d99a1267"}},"385d77d6d3f3433dba7abee844aa9a10":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_51147d83c4aa4c5bbffbc6a74f9c955e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_218a0e4386434ace89051146f62e749a"}},"36f815ff8e6d45f488f5c4734c039f6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e0574f58b42d47af870972c64d02d3a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00,  6.31ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2af420159b145a8aaeec1ff72b2aa43"}},"1eb4e46b44a2461eb06781f34d799996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ce166393d8ff47ac894262b7d99a1267":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"51147d83c4aa4c5bbffbc6a74f9c955e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"218a0e4386434ace89051146f62e749a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e0574f58b42d47af870972c64d02d3a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a2af420159b145a8aaeec1ff72b2aa43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fROlRPYfIXxo","executionInfo":{"status":"ok","timestamp":1630448318327,"user_tz":-60,"elapsed":5907,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"b5c6ce90-3d23-4eb0-a497-55f86928a398"},"source":["! pip install transformers\n","! pip install datasets"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.11.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.16)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.8.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684,"referenced_widgets":["f1e1cb236c5445d0818093a00a4cfbf7","abf37574657f4268a66f2312bfb9c0b8","af7a4c38227d4c438f8ca56fedfc1ca1","9bc127ef53ad4ac6a83dffa27917e263","6a8087213b6e4138bf810367fddc2c65","953498b9fcdd452288fdf2e5898d5f9b","29feaa827c6640a0820fe37886b79283","f0bdc1e6850f4b3594582057635a572d","3ae8cb2534144f608db50fa82c8a8b05","23066135b94648119b9ca60e1d8be1aa","f45f0ebf4ec5483db7449828f9e31939","d7386f610b364b188e4e13d8d9b3b4ea","5269ee0710104db7b0a15de0a4c220db","56481dcc14d64463a11e6b2687bc5fc9","522db33f44bc4d0ba75f71c295e9c25d","86546f608d3c41198e73d7063d887020","1d1ab85e2f5d4ffc8a52206cb96554c6","7fcb5a4639364f06a2c78b079ec5f9c2","1166029d1bbb4ac293dc003836bda2c1","9bd9bfc281df4ffe9f8a3b8aa82b7e43","083b3697cc5d46049c307f5ec8a159ac","5be116338ab44c6b8b4f64e45b7672e9","00812e0c345249a7ac3174ffa48e9318","ded69d9e840d4ee1a16cbccfc2f81d77","86430a5eb5a8418fa6e632e1f82eba5f","cb3b010dc01f40a0a109aef05907a893","19251edf6c6241a7a98bff18fa962008","2ebabfffcfe34a7dafe6f1bfa196195b","de6f54b24fb54a71b22bdef6ca0db9ed","a9db5deb5281493584003619e2480289","f0d73364594c42b088ca630cc43cc574","6966e7c2f8d94f3da17e7a977171fbb6","4f75cf6318714111899b1b0fe394572e","0584ccd388ea4ea0949b92dbbb8cde98","f9b6d65f2d0f4929bc156bbc23925bd5","4ead017c97a44e389c43cd79ec4db3e2","24603dce67da4a88a356d70ab3181f32","b403a767985b44539f7557b4e53fba04","57ce14013494470ca1350e2f579b6599","0547d4a4a3ff4d38b31de76a870ad44b","4b9ca0d4205146d5bcc35d7911cb5f38","fc45ec00c8e147399c80fb159fedfc79","bc52fa6b757c4b279327b7ffedc9c598","5f73177b70c34f3c8bc597c7cf15a220","5dbe0e1eb79d45d280554a5d11fe1894","4699120e2da647bc8dd9516df7b8f814","87f8165e326740e69ba06dce9ea1c198","37e22b06c9104a169a6709d90144549c","0f93d143c4aa43a1add2e34cf7bab555","241a936eeacc45fd8859092702b40546","f7ccebf7a03d4387b54d4535d1875510","abe1e73b86714d18b12acacaa36a7d97","12a38b9bb7d94ea2b8e274f99551f118","6ffe43a99d484adfbc973e8ace61c22d","34a827c9376c4adbacdf50d14560c9cb","380f41d4b5fc45669eaa33ee7d7f4de8","4d393b31f8f5461ab462ce715dd4c1d5","76963d58ef314adaa45053f6f81fba58","b1996138da594eacb792f5d8235f677d","2413bf0e8f89443cb57e2c4ed765929d","804397f9856e47a1bd95a6955d086b71","0159d192204a4df1bfda778e01bccab8","d19da038749a46c58adfab9bdd84f755","d7af48fc401049cc9d1ad0af9cbb6aeb","21053245339c4a6e80faaa14e632d3f3","22d4869562d14d09ac93f04819d8f4ab","4e9ff8eb8aba43779f23812680ccdc00","df1a60fbf26f4a42bf1016521f5943fc","69a571979e524735ba7cc237c20c7d86","3fbf32e309954156966379d7b12b4afe","52a8a7166b1b4ec7b6d65819d3734a8c","75bff487a489477fa382e683badbdc00","8f1f1df376a448ee9c59b0f7cd9a32af","b194038990384a73b448eb94944f7fb0","6084c6e7931f49448da4ea02350ade24","e88166f843e74fde933e8a342cb58d26","29bda4046c8248f898c247193e228278","3373e993f873445f884c5d200dab6c60","ad1ebd71d23e41bbb89df69e53b72625","df023b473c8b488489a57b60c6f58771","2a32c3e5cfa5467ea726807e55f8768a","6262a5ae17984c278930895557485916","547e922454664b9a9794b10eb01ec130","7cf4fefffed74f14a8c31c1b5e3193f4","fc405cb0d4334586a4dfa8d5b61ef806","7ac7a92706964798a18e1ddf421ca372","ddf1de32afa84df59e5891185aab9ac7","1ae8af5f82ff4ecdb7151c2b18fb28f1","a2fa2a983cd940599bc07c11c3f0a1d6","dc2ebdcb64c44f2f985614baa3046edf","158e75a57a0c43ce8a905b0e0c2b501e","840fbd9c0ab648e192e8e444d67959fb","e7cc7aad7e15451c853e605e3930a40d","b26221a057364eb5ad8cdebb185d012e","4df1767e61da422eb01edc49a03bda17","ba62297eec7c45b1be8cfaca10f42138","c404eae477d443db8efb30ea04eab5a0","e07fddaef3ca491bbaa3a77688f22c5e","06eaa66cb95546008dba5bac52daf3c3","ea63e8357ab9441e9e745f3f70cdeefd","5813b5f27b7c4cc68882a84f945739ee","81c223da20e94f698f1823307e5a52de","6c0aecfccb2a49019cbb6d247a442b5d","035b2523d0644f68935567225b086506","488e17fe057d490e98f1da0b1017db77","49e00b3026434d52a9616193238d1ead","7e85ce0e01fd46a382fb5510f9408315","e45b67bd2ae242eea21928b72b3664de","fa7893f41c924b1183150247a77ceecd","83c3a4e019c6472ca93b6c10bd574428","dad23542c4a24921b4e671daa46b43aa","1b489018650d430c82548faf2e0c8803","13dd983edcc34812b23e909b89ce0be3","4269328580794a798273da6c5685f044","73fbcd4adf3d47de984ccc10cfa3728e","afdf47369e764964be8dd08c8ec6a253","1340aa8c26d745598d599bb2875c81af","7dc4ab720b4a448cb82b230f3629ea3c","6283f8ea2bce44b8bdd99cb57dd5b2f4","0bab3e2f3de34c818fc5ebce926f6d6d","255d383015474276beed4e3f49c45c39","cb87018c5f4e43d5b7c436feb368df97","61198616a90a48869ef1a34736b3346c","3054bd8bccfc48dbb766b16701489ce5","fe9efb2f4cbe481b8e7c1f0b29dcc618","e360499b81e8410facac12bab61d8908","923e0cdb856845099de1ac62945a70d0","9753296db7d14f5ca8a96ed7ba41eaa2","df7363944069405d92f3dbc2a60c65bc","02026c902fb745328c5fbdfd8ef5dbfa","8c95ec28294f4cc5bb6c5e11d9083bf6","8705bac0e739489a9e066c58958ea59f","a802a5463a9f411688b9177d776b2b2c","4729f6a1cd8d449082f9b32e89729bcc","168dae62475a43d8a14f1a7095410f77","253c0a325e5247118031c2c2ff68c261","3445cea26a56488aab5f555eb702c641","1a487301fb1b4369ac717ce372524367","fae6d1237da142cbb9f1bcc3828cfb99","f2a3225f68464e6a81d22f3a790cd012","e0a7f57aae334fc9a5c68fad2026eed9","33986c667b0341eb90711d76f155d570","0028d3890b8c471f8c50d15fdc179691","29443d4588ce4b32874e47d151875c48","29117fe03c8b4545aaf3eba77405e520","ef0aa77e6615496cad6bf9a4c1f64417","a2178f2938dc4b8d97c08526ac7fef97","bff13f580c6144daa76efd616ae258ab","87909fa20eaa4a1f94cc29529175af28","9db3bdb0f5174c85a427b0b09d1ec5c4","0e1c553dd61745a9bf23863db1093c2a","7f1044d64f484476be6630755626fc5f","9a336049d7494f279956ad70086cab77","4d1ba0a6a69848aa88beec0f88582511","8cdc8faa31f34356bbc5bf47e77d3b4e","c7e38a6a2f794f7dad0024f76778567c","4d5a4587bea845dba7f1001db16fd1dc","8f96b3de509741afa3fcebd4b3426055","9ed3665cb4df46daa728623a3e1d0b26","568884ad4ea845fbbd07f3d5f3ad6c61","338f113de12d441f87ef8ba2f7cc80a7","aae4ebc6a37d4f62b27e2c5cbf9f7a9c","219ef01dbdfb4b689b04bb6810da77b2","2ad8d6b02b3e47e58d942311f9563378","1c05eedcbc524e2c8e246d3c46114b06","cfc0e4bd83c14477be16730aefaae7b7","81f958dd376e48deb692d7a7db6a7446","92f5180c1d4f412388fd6516d5eacec5","7fa178775a5b42db87318d435a7861f9","5556be700e15469d8b1cfe548cc69cd9","98130681e1bc4868b3200aa144179f59","fd691601f90c48d09f31b4eb34c0d4fc","e6e2dc53db0b44e5970166eeee300ba9","2f639400ddcf433c85d55177a3b8c564","e086d545b6fa4aa3b9ab36e223b807f9","1e360034823c4e07824669296120b0c3","1dd8378bacba4a01aa2c9eb8ed558b20","b590ed2573b74752a930b02e341d8a8b","e38832a548884c1eb2d4b216e36fe41f","69a899519b704dd58e1a4fc3616864af","f7db5430ed1140829f29ba9aaf8d650d","0c26603fd51d4e92bcceb556c111ea16","72eb8e63bb2a47f79be4dbdadd26276c","d791948f62dd49978f8914866fcb5cbc","6c79d3b7a9b348f5b634655387014dcb","07d8939e62734b7980cc6c2b29c67257","e57c6133dca44cd494f42d9949e0e566","83a849b93f8f4ccfae91349fda05485b","145029449e2743089e0357e31412e019","467895bbd1784db5bdb8cb2bc7589053","385d77d6d3f3433dba7abee844aa9a10","36f815ff8e6d45f488f5c4734c039f6a","1eb4e46b44a2461eb06781f34d799996","ce166393d8ff47ac894262b7d99a1267","51147d83c4aa4c5bbffbc6a74f9c955e","218a0e4386434ace89051146f62e749a","e0574f58b42d47af870972c64d02d3a7","a2af420159b145a8aaeec1ff72b2aa43"]},"id":"XEIM6f28Inyl","executionInfo":{"status":"ok","timestamp":1630448374307,"user_tz":-60,"elapsed":56002,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"75841a20-066e-4d86-cb96-1e361d795aa8"},"source":["# imports\n","import pandas as pd\n","import numpy as np\n","import pdb\n","import os\n","os.chdir('/content/drive/MyDrive/CAMemBERT2')\n","import re\n","from datasets import Dataset,DatasetDict,load_dataset,concatenate_datasets\n","from transformers import AutoTokenizer\n","from torch import FloatTensor\n","from torch.cuda import is_available\n","from math import ceil,floor\n","import random\n","import json\n","import time\n","\n","# A class that merges the fce grammatical error detection (ged) dataset to fce automated essay scoring dataset (aes) \n","# so that grammar tags and essay scores can be used.\n","# for further notes of how this class works and additioanl essay matching methods see bottom\n","class LinkGedDatasetToEssayDataset:\n","\n","    # mapping for string labels to numerical ones in ged dataset\n","    _map_labels_2_ids = {'c':0,'i':1}\n","\n","    def __init__(self,set_type='train'):\n","        # counter for the number of differences where characters appear in the aes dataset but not ged dataset\n","        self.errors_count=0\n","        # set type can be train, test, dev\n","        self.set_type = set_type\n","        self.essays = self.read_and_parse_essay_data()\n","        # create mapping of column. names to indexes for itertuples iteration in self.get_differences() method\n","        self.essay_col_index = {col:i+1 for i,col in enumerate(self.essays)}\n","        self.ged = self.read_and_parse_ged_data()\n","        # string containing all words in ged dataset with no whitespace\n","        self.all_words_no_ws = ''.join(self.ged.word.tolist())\n","        # final dataframe with essay text, essay scores, essay ids and essay grammar labels \n","        self.updated_df = self.match_essays_and_grammar_labels()\n","\n","    def get_updated_df(self):\n","        return self.updated_df\n","\n","    def read_and_parse_essay_data(self):\n","        essays = pd.read_json(f'data/fce.{self.set_type}.json',lines=True)\n","        essays['text'] = essays.text.str.replace('\\n',' ')\n","        essays['text_no_ws'] = essays.text.str.split().str.join('')\n","        essays['essay_char_len'] = essays['text_no_ws'].apply(len)\n","        essays['end_word_ind'] = essays['essay_char_len'].cumsum()\n","        essays['start_ind'] = essays['end_word_ind'] - essays['essay_char_len']\n","        essays['ind_combined'] = essays.apply(lambda x: list([x['start_ind'],x['end_word_ind']]),axis=1)\n","        return essays\n","\n","    def read_and_parse_ged_data(self):\n","        ged = pd.DataFrame(pd.read_csv(f'data/fce-public.{self.set_type}.original.tsv',sep='  ',names=['word']).word.str.split('\\t',1).tolist(),columns = ['word','labels'])\n","        ged['labels'] = ged['labels'].map(self._map_labels_2_ids)\n","        ged['end_word_ind'] = ged.word.apply(len).cumsum()\n","        return ged\n","\n","    def match_essays_and_grammar_labels(self):\n","        # essays_to_keep = index of all essays that form an exact match with all words in the ged dataset string\n","        essays_to_keep,matched_essays = zip(*[(i,re.search(re.escape(essay_no_ws),self.all_words_no_ws)) for i,essay_no_ws \n","                                                      in enumerate(self.essays['text_no_ws'].tolist()) \n","                                                      if essay_no_ws in self.all_words_no_ws ])\n","        # get the words from the ged dataset (with whitespace) to form each essay\n","        essays_from_ged = [' '.join(self.ged.loc[(self.ged['end_word_ind']>m.start())& (self.ged['end_word_ind']<=m.end())]['word'].tolist()) for m in matched_essays]\n","        # get the labels from the ged dataset corresponding to each essay\n","        labels_from_ged = [self.ged.loc[(self.ged['end_word_ind']>m.start())& (self.ged['end_word_ind']<=m.end())]['labels'].tolist() for m in matched_essays]\n","\n","        # make sure essays dataframe only contains essays that have matched\n","        essays = self.essays.iloc[list(essays_to_keep)].reset_index()\n","        return pd.concat([pd.DataFrame({'text':essays_from_ged,'labels':labels_from_ged}),essays[['answer-s','script-s','id']]],axis=1)\n","\n","\n","# Class to create a huggingface FatasetDict for the fce dataset for tasks of aes and ged\n","class CreateHuggingFaceDictFce:\n","\n","    # class varaibles \n","    # possible set types\n","    _set_types = ['train','dev','test']\n","    _cols_to_keep = ['attention_mask','labels','input_ids','scores']\n","    _answer_score_mapping = {\n","                      0.0:0,\n","                      1.1:1,1.2:4,1.3:8,\n","                      2.1:9,2.2:10,2.3:11,\n","                      3.1:12,3.2:13,3.3:14,\n","                      4.1:15,4.2:16,4.3:17,\n","                      5.1:18,5.2:19,5.3:20,\n","                  }\n","\n","    def __init__(self,pretrained_model= 'distilroberta-base',max_length=512,scoring='script'):\n","        # max length for tokenization\n","        self.max_length = max_length\n","        # huggingface tokenizer\n","        self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n","        self.scoring = scoring\n","        self.fce_df = self.parse_data()\n","        self.dataset_dict = self.create_hf_dataset_dict()\n","        self.set_weights()\n","\n","    def get_df(self):\n","        return self.df_for_sent\n","\n","    def get_dataset_dict(self):\n","        return self.dataset_dict\n","\n","    def get_weights(self):\n","        return self.class_weights\n","\n","    def parse_data(self):\n","        # create one dataframe containing all samples from all train, test and dev set \n","        combined_df = pd.concat([self.add_col(LinkGedDatasetToEssayDataset(set_type).get_updated_df(),'set_type',set_type) for set_type in self._set_types],axis=0)\n","        if self.scoring == 'script':\n","            df = combined_df.rename(columns={'script-s':'scores'})\n","            df['scores'] = df['scores'].astype(float)\n","            # group all essays by id and combine the text and labels as well as merge the scores and set types        \n","            df = df.groupby('id').agg({'text':list,'labels':list,'scores':list,'set_type':list})\n","            df[ 'text' ] = df[ 'text' ].str.join(' ')\n","            df['labels'] = df['labels'].apply(lambda x : x[0] + x[1] if len(x)>1 else x[0])\n","            df[ 'scores' ] = df[ 'scores' ].apply(lambda x : x[0])\n","            df[ 'set_type' ] = df[ 'set_type' ].apply(lambda x : x[0])\n","        elif self.scoring == 'answer':\n","            df = combined_df.rename(columns={'answer-s':'scores'})\n","            # correct for errors in scoring\n","            df[ 'scores' ] = df[ 'scores' ].str.replace( '/','.' ).str.replace('T','')\n","            # remove values containing non-numeric data and are in score mappings\n","            df = df[ ~pd.to_numeric( df[ 'scores' ] , errors='coerce' ).isna() ]\n","            df = df[ df['scores'].astype(float).isin(self._answer_score_mapping.keys())]\n","            # map scores to new values\n","            df[ 'scores' ] = df[ 'scores' ].astype(float).map(self._answer_score_mapping)\n","            # return only certain columns from the original dataset\n","        return df.reset_index()[['text','labels','scores','set_type']]\n","\n","    # used to add set type column to each of the train test and dev dataframes\n","    def add_col(self,df,col,val):\n","        df[col] = val\n","        return df\n","\n","    def create_hf_dataset_dict(self):\n","        # create a hugging face dataset for each of the train test and dev samples and combine them to create a huggingface dataset dictionary\n","        dataset_dict = DatasetDict({set_type:Dataset.from_pandas(self.fce_df.groupby('set_type').get_group(set_type)) for set_type in self._set_types})\n","        # apply the method to extend the labels for grammatical error detection and tokenize each essay\n","        dataset_dict = dataset_dict.map(self.extend_labels_for_tokenizer).map(self.tokenize_text)\n","        # find the columns to drop from the new dataset dict\n","        cols_to_drop = set(dataset_dict.column_names['train']) - set(self._cols_to_keep)\n","        return dataset_dict.remove_columns(list(cols_to_drop))\n","\n","    def extend_labels_for_tokenizer(self,example):\n","        # split text by white space to create individual tokens that correspond to labels\n","        tokens,labels = example['text'].split(),example['labels']\n","        labels_for_tokens = [] \n","        # split word counts (words that are plit up by tokenizer)\n","        split_count = 0\n","        # iterate through each token generated when the tokenizer is applied to the full length text\n","        for index, token in enumerate( self.tokenizer.tokenize( ' '.join( tokens ) , truncation = True , padding = False , add_special_tokens = False , max_length = self.max_length ) ):\n","            # if conditions to determine if the tokenizer has split a word based on tokenizer used\n","            if ( ( ( ( token.startswith( \"Ġ\" ) == False and index != 0 ) or ( token in tokens[ index - split_count - 1 ].lower() and index - split_count - 1 >= 0 ) ) and self.tokenizer.sep_token == '</s>' ) \n","                or ( ( token.startswith( \"##\" ) or ( token in tokens[index - split_count - 1].lower() and index - split_count - 1 >= 0 ) ) and self.tokenizer.sep_token == '[SEP]' ) ):\n","                # add a padding token for words that are split by the tokenizer\n","                labels_for_tokens.append( -100 )\n","                # add a count \n","                split_count += 1\n","            else:\n","                # add the label to all tokens that either haven't been split by the tokenizer or are the first word of a split\n","                labels_for_tokens.append(labels[index - split_count])\n","        # pad and truncate the labels to be the max length of the tokenizer by padding -100 to the token length where necessary\n","        return {'labels':np.pad( labels_for_tokens , ( 0 , self.max_length - len( labels_for_tokens ) ) , 'constant' , constant_values = ( 0 , -100 ) )[:self.max_length]}\n","\n","    # get the padded and truncated input ids and attention masks for each text (essay)\n","    def tokenize_text(self,example):\n","        return self.tokenizer( example['text'] , truncation=True , padding = 'max_length' , max_length = self.max_length )\n","\n","    # set weights to apply to the cross entropy loss function to penalise for under represented classes\n","    def set_weights(self):\n","        dataset = self.get_dataset_dict()\n","        padding,n_c,n_i = np.unique(np.concatenate(dataset['train']['labels']),return_counts=True)[1]\n","        class_weights = FloatTensor([(n_c + n_i)/(2.0 * n_c),(n_c + n_i)/(2.0 * n_i)]).to('cuda' if is_available() else 'cpu')\n","        self.class_weights = class_weights\n","\n","# Class to create a huggingface DatasetDict for the conll2003 dataset for task of Named Entity Recognition NER\n","class CreateHuggingFaceDictNerandAesDataset(CreateHuggingFaceDictFce):\n","\n","    # class variables\n","    # possible set types\n","    _cols_to_keep_before_dataset_conversion = ['tokens','ner_tags']\n","    ner_dataset = \"conll2003\"\n","\n","    def __init__(self,pretrained_model= 'distilroberta-base',max_length=512,scoring='script'):\n","        super().__init__(pretrained_model,max_length,scoring)\n","        # load in ner dataset from huffingface\n","        dataset = load_dataset(self.ner_dataset).map(self.append_sep_and_pad)\n","        self._set_types_ner = list(dataset.keys())\n","        dataset_dict = self.reshape_to_match_to_fce_dataset(dataset)\n","        # apply tokenization\n","        dataset_dict = dataset_dict.map(self.extend_labels_for_tokenizer).map(self.tokenize_text).remove_columns(['text'])\n","        self.dataset_dict_ner = dataset_dict\n","\n","    def get_ner_dataset_dict(self):\n","        return self.dataset_dict_ner\n","\n","    def get_fce_dataset_dict(self):\n","        return self.dataset_dict\n","\n","    # append a sep token to the end of each sample from the ner dataset and a padtoken to the end of each label\n","    def append_sep_and_pad(self,example):\n","        tokens,tags = example['tokens'],example['ner_tags']\n","        tokens.append(self.tokenizer.sep_token)\n","        tags.append(-100)\n","        return {'tokens':tokens,'ner_tags':tags}\n","\n","    def reshape_to_match_to_fce_dataset(self,dataset):\n","        dataset_dict = {}\n","        # variable that dictates the length of tokenization in the ner dataset so that the number of samples/rows match that of the fce dataset\n","        self.max_length_for_training = None\n","        # iterate through the set types of the ner and fce datasets\n","        for set_type,set_type_fce in zip(self._set_types_ner,self._set_types):\n","            # list_for_each_set_types_data\n","            type_data = []\n","            # iterate through each column that is needed from the ner dataset\n","            for col in self._cols_to_keep_before_dataset_conversion:\n","                # flatten all the values in the curent column to a 1d array\n","                flattened_values = np.concatenate(dataset[set_type][col])\n","                # if the set type is train clalculate the length of tokenization required to ensure the number of rows in the ner dataset \n","                # is equal to the number of rows in the fce\n","                if set_type=='train':\n","                    split_arr = np.array_split(flattened_values,self.get_fce_dataset_dict()[set_type_fce].num_rows)\n","                    if self.max_length_for_training==None:\n","                        max_row_length_of_split_array =  max([len(row) for row in split_arr])\n","                        self.max_length_for_training = max_row_length_of_split_array if max_row_length_of_split_array <= self.max_length else self.max_length\n","                # if the dataset is not train then set the dataset to have the same max length tokenization as max_length_for_training\n","                # unless max length for training is greater than max length of tokenizer in which case the max_length of tokenizer should be used\n","                else:\n","                    # reshape array to have the correct max length of tokenization but an unlimited number of rows\n","                    length_of_new_arr = ceil(len(flattened_values)/self.max_length_for_training )\n","                    split_arr = np.array_split(flattened_values,length_of_new_arr)\n","\n","                # pad each row in the split array to have the same length\n","                padded_array = []\n","                for row in split_arr:\n","                    if col == 'tokens':\n","                        padded_array.append(' '.join(list(np.pad(row,(0,self.max_length_for_training -len(row)),constant_values = (self.tokenizer.pad_token,self.tokenizer.pad_token))[:self.max_length_for_training])))\n","                    else:\n","                        padded_array.append(list(np.pad(row,(0,self.max_length_for_training -len(row)),constant_values = (-100,-100))[:self.max_length_for_training ]))\n","                type_data.append(pd.Series(padded_array))\n","            # create a hugging face dataset from the set type\n","            tmp_df = pd.DataFrame({'text':type_data[0],'labels':type_data[1]})\n","            dataset_dict[set_type] = Dataset.from_pandas(tmp_df)\n","        # combine all data into datasetdict\n","        return DatasetDict(dataset_dict)\n","\n","# to note:\n","# \t\t\t- ner training dataset is padded to be the same width and length as the fce dataset so that it can be loaded into a model without the need for over\n","#\t\t\t  under sampling. \n","class CreateHuggingFaceMultiTask(CreateHuggingFaceDictNerandAesDataset):\n","\n","    # class variables\n","    _col_to_add_to_ner = ['score']\n","    _set_types = ['train','test','dev']\n","    _map_fce_set_to_ner = {'train':'train','test':'test','dev':'validation'}\n","    _map_ner_set_to_fce = {'train':'train','test':'test','validation':'dev'}\n","    _tasks = ['aes','ged','ner']\n","\n","    def __init__(self,pretrained_model='distilroberta-base',max_length=512, scoring='script',batch_size=8):\n","        super().__init__(pretrained_model,max_length, scoring)\n","        self.fce_dataset_dict = self.get_fce_dataset_dict()\n","        self.ner_dataset_dict = self.get_ner_dataset_dict()\n","        self.batch_size = batch_size\n","        for set_type in self._set_types:\n","            # add a dataset column to fce dataset\n","            self.fce_dataset_dict[set_type] = self.fce_dataset_dict[set_type].add_column('dataset',[0]*self.fce_dataset_dict[set_type].num_rows)\n","        for set_type in self._set_types_ner:\n","            # add a score column to ner dataset and pad values \n","            self.ner_dataset_dict[set_type] = self.ner_dataset_dict[set_type].add_column('scores',[-100]*self.ner_dataset_dict[set_type].num_rows)\n","            # add a dataset column to ner dataset\n","            self.ner_dataset_dict[set_type] = self.ner_dataset_dict[set_type].add_column('dataset',[1]*self.ner_dataset_dict[set_type].num_rows)\n","            # make sure all columns are of the same dataset type\n","            self.ner_dataset_dict[set_type] = self.ner_dataset_dict[set_type].cast(self.fce_dataset_dict[self._map_ner_set_to_fce[set_type]].features)\n","        # combine the two datasets through concatanation\n","        self.combined_dataset_dict = self.combine_datasets()\n","\n","    def get_combined_dataset_dict(self):\n","        return self.combined_dataset_dict\n","\n","    # generate a dataloader for training and testing models so that data is loaded in alternating tasks for training \n","    # and all samples from one task then another for testing and dev.\n","    def combine_datasets(self):\n","        dataset_dict = {}\n","        for set_type in self._set_types:\n","            # concatanate datasets so one follows another from a list of all the datasets\n","            dataset_lst = [self.get_fce_dataset_dict()[set_type],self.get_ner_dataset_dict()[self._map_fce_set_to_ner[set_type]]]\n","            concatenated_datasets = concatenate_datasets(dataset_lst)\n","            if set_type=='train':\n","                # get the length of each dataset\n","                lengths = [dset.num_rows for dset in dataset_lst]\n","                # get the offset for each dataset (number of samples between the begining of the concatanated dataset and the start of a new dataset)\n","                offsets = np.cumsum([0] + lengths[:-1])\n","                # get a list of indexes for the minimum length dataset (although both the same length)\n","                indexes = list(np.arange(min(lengths)))\n","                # get a list of all the possible indexes in the smallest / first dataset\n","                indicies = [offset + indexes for offset in offsets]\n","                # list for storing the order which batches should appear in\n","                batch_order=[]\n","                for _ in range(ceil(min(lengths)/self.batch_size)):\n","                    # create a list of mini batch indexes by appending randomly sampled indexes of length batch size \n","                    # until they run out / can no longer fill a batch and then append the remaining to the last batch\n","                    # (this is samples only for the smallest / first dataset)\n","                    try:\n","                        samples = random.sample(indexes , self.batch_size)\n","                        batch_order.append(samples)\n","                        indexes = [ind for ind in indexes if ind not in samples]\n","                    except:\n","                        batch_order.append(indexes)\n","                # extend samples to both datasets\n","                bath_indexes = [[ind[mini_batch_inds] for ind in indicies] for mini_batch_inds in batch_order]\n","                # flatten out the list of lists (potential for one mixed batch that will be handled by model)\n","                batches_flattened = [list(mini_batch) for mini_batches in bath_indexes for mini_batch in mini_batches]\n","                # select samples in order defined above\n","                dataset_dict[set_type] = concatenated_datasets.select( np.concatenate(batches_flattened) )\n","            else:\n","                dataset_dict[set_type] = concatenated_datasets\n","        return DatasetDict(dataset_dict)\n","\n","\n","\n"," \n","\n","\n","# further notes:\n","#           - each row in the ged dataset represents one token and its corresponding error label, \n","#             where as one row in the aes dataset represents one essay and its score.\n","#           - each token has been formed using rasp tokenization.\n","#           - words in ged dataset appear in the same order as they do in essay dataset.\n","#           - some essays in the ged dataset contain extra words or miss words when compared to the original dataset.\n","#             these are omitted from the dataset. \n","# Works by: \n","#           - indexing the end of every word in the ged dataset\n","#           - joining all the words in the ged dataset together with no whitespace \n","#             (creating one string of all the words in the ged dataset (referred to as ged text))\n","#           - joining all the words in each individual essay together with no whitespace\n","#           - locating where the essay with no whitespace matches the sequence of words in the joined ged text \n","#           - use the index of the start and end word of the appearance of the essay in the ged text as a way to locate the rows in the ged dataset \n","#             corresponding to the words in an essay, these rows to get the error labels and words for each essay and merged with essay score and grammar labels.\n","# Further notes:\n","#           - however, this does mean the tokenization used to split up words in the essay by the ged dataset does affect the final appearance of the essay.\n","#             (as more whitespace appears due to splitting of individual words by the tokenizer in the ged dataset), which was seen to negatively impact essay predictions.\n","#           - but, using the original essay leads to incorrect tagging of the words in each essay; as a result of labels needing to be extended for transformer tokenization.\n","#             (which was seen to negatively impact grammar predictions).\n","#           - the impact of incorrect tagging was considdered to be of great impact to the overall validity of the model, hence the joined words from the ged\n","#             were used as opposed to using the original words dataset.\n","#           - attempts at making the essays and tags match exactly by locating differences in between the two datasets were somewhat but were at risk of error\n","#             so there were 70 scripts omitted from the original training set, 7 from the original developement set and 9 from the original test set.\n","#             Additionally, attempts to perform exact matches were futile for the test dataset as essays in the aes dataset appeared in a different order to the ged dataset. \n","\n","# Following code block omitted from experimentation due to possible errors\n","    ###################################################################################################################################\n","class LinkGedDatasetToEssayDatasetWithAdditionalMatching(LinkGedDatasetToEssayDataset):\n","\n","    def __init__(self,set_type='train'):\n","      super.__init__(set_type)\n","\n","    def additional_matching(self):\n","        # create a list of essays that match between the essay and grammar data by using the start and end index of the essay \n","        # as they appear in the aes dataset.\n","        # done by : using the cumulative sum of the length of each essay to get indexes in the aes dataset.\n","        # then using the indexes to find the differences where characters appear in the aes dataset but not the ged dataset\n","        # the number of differences are used to adjust the indexs of the start and end of each essay for indexind of the ged dataset.\n","        # this method was not yetd eveloped to remove characters that appear in the grammar dataset but not the essay dataset.\n","        self.find_differences()\n","        rows_from_ged = [self.ged.loc[(self.ged['end_word_ind']>m[0])& (self.ged['end_word_ind']<=m[1])] for m in self.new_indexes]\n","        # get the words from the ged dataset (with whitespace) to form each essay\n","        essays_from_ged = [' '.join(tmp_df['word']) for tmp_df in rows_from_ged]\n","        # get the labels from the ged dataset corresponding to each essay\n","        labels_from_ged = [tmp_df['labels'] for tmp_df in rows_from_ged]\n","        return pd.concat([pd.DataFrame({'text':essays_from_ged,'labels':labels_from_ged}),self.essays[['answer-s','script-s','id']]],axis=1)\n","\n","    def find_differences(self):\n","        # store list of text for essays with differences removed\n","        self.new_text = []\n","        # store new indexes for essays with differences removed\n","        self.new_indexes = []\n","        # store list of indexes of differences\n","        self.new_indexes_errors = []\n","        # iterate through the aes dataset with itertuples\n","        for i,row in enumerate(self.essays.itertuples()):\n","            # get essay with no whitespace and essay length (characters) from the aes dataset\n","            aes_essay_no_ws , char_len = row[self.essay_col_index['text_no_ws']] , row[self.essay_col_index['essay_char_len']] \n","            # get start and end of essay as it appears in the aes set \n","            # but with the number of differences that have appeared so far between the two datasets subtracted from the start and end index\n","            start_ind , end_ind = row[self.essay_col_index['start_ind']]-self.errors_count,row[self.essay_col_index['end_word_ind']]-self.errors_count\n","            # use the start and end index to get the potentially matching characters between the ged and aes datasets \n","            ged_essay = self.all_words_no_ws[start_ind:end_ind]\n","            # method to locate errors between the two datasets\n","            self.locate_chars_in_aes_but_not_ged(aes_essay_no_ws,ged_essay,char_len,start_ind,end_ind)\n","\n","    def locate_chars_in_aes_but_not_ged(self,aes_essay_no_ws,ged_essay,char_len,start_ind,end_ind):\n","        # if the two essays are equal append the indexes to the new_index list\n","        if aes_essay_no_ws==ged_essay:\n","            self.new_indexes.append([start_ind,end_ind])\n","        else:\n","            # counter for number of characters in aes essay but not the ged dataset\n","            aes_essay_errors_count = 0\n","            # find the first character in aes essay but not the ged dataset\n","            current_error = min(np.nonzero(np.invert(np.array(list(aes_essay_no_ws))==np.array(list(ged_essay))))[0])\n","            # get the text which is in the ged essay up to the first difference \n","            correct_text = ged_essay[:current_error]\n","            more_errors = True\n","            while more_errors==True:\n","                # get the characters in the the aes essay beyond the current character which is found to be a difference between the two datasets\n","                tmp_aes_essay = aes_essay_no_ws[current_error+aes_essay_errors_count+1:char_len]\n","                # get the characters beyond the current difference in the ged essay\n","                tmp_ged = ged_essay[current_error:char_len-aes_essay_errors_count-1]\n","                if tmp_aes_essay == tmp_ged:\n","                    # update the corrected text to contain the ged essay text before the current error\n","                    correct_text = correct_text + tmp_ged\n","                    # add the number of errors to the errors count\n","                    aes_essay_errors_count = aes_essay_errors_count if aes_essay_errors_count!= 0 else 1\n","                    self.errors_count+=aes_essay_errors_count\n","                    more_errors = False\n","                    break\n","                else:\n","                    # find the number of characters between the current difference and next one\n","                    char_to_next_error = min(np.nonzero(np.invert(np.array(list(tmp_aes_essay))==np.array(list(tmp_ged))))[0])\n","                    # get the index of the next difference in the essay \n","                    current_error += char_to_next_error\n","                    # update the corrected text to contain the ged essay text before the current error\n","                    correct_text = correct_text + tmp_ged[:char_to_next_error]\n","                    # add an error to the current essay difference count\n","                    aes_essay_errors_count+=1\n","    ###################################################################################################################################\n","\n","dataset_obj = CreateHuggingFaceMultiTask()\n","dataset_dict = dataset_obj.get_combined_dataset_dict()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1e1cb236c5445d0818093a00a4cfbf7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1057 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7386f610b364b188e4e13d8d9b3b4ea","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/97 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"00812e0c345249a7ac3174ffa48e9318","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/79 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0584ccd388ea4ea0949b92dbbb8cde98","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1057 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dbe0e1eb79d45d280554a5d11fe1894","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/97 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"380f41d4b5fc45669eaa33ee7d7f4de8","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/79 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"stream","text":["Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e9ff8eb8aba43779f23812680ccdc00","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/14041 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3373e993f873445f884c5d200dab6c60","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3250 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a2fa2a983cd940599bc07c11c3f0a1d6","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/3453 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ea63e8357ab9441e9e745f3f70cdeefd","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1057 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dad23542c4a24921b4e671daa46b43aa","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/266 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cb87018c5f4e43d5b7c436feb368df97","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/243 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a802a5463a9f411688b9177d776b2b2c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1057 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29443d4588ce4b32874e47d151875c48","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/266 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cdc8faa31f34356bbc5bf47e77d3b4e","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/243 [00:00<?, ?ex/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cfc0e4bd83c14477be16730aefaae7b7","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1dd8378bacba4a01aa2c9eb8ed558b20","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83a849b93f8f4ccfae91349fda05485b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"2nmRDRcZwR40"},"source":["import pandas as pd\n","import numpy as np\n","import pdb\n","import os\n","os.chdir('/content/drive/MyDrive/CAMemBERT2')\n","import re\n","import string\n","import copy\n","from transformers import AutoModelForSequenceClassification,TrainingArguments,Trainer,AutoTokenizer,AutoModelForTokenClassification,AutoModel,AutoConfig,EarlyStoppingCallback\n","from transformers.models.bert.modeling_bert import TokenClassifierOutput\n","from datasets import DatasetDict,Dataset,load_dataset\n","from scipy.stats import spearmanr\n","from sklearn.metrics import cohen_kappa_score\n","from math import floor\n","import torch \n","from torch import nn\n","\n","# class ClassificationHead(nn.Module):\n","\n","#     def __init__(self,task_name,mini_task_dict,pretrained_model_name='distilroberta-base',shared_encoder_layer=None):\n","#         super().__init__()\n","#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#         model = mini_task_dict['model'].from_pretrained(pretrained_model_name,\n","#                                                         num_labels=mini_task_dict['n_labels'],\n","#                                                         output_hidden_states=True)\n","#         self.dropout = nn.Dropout(model.config.hidden_dropout_prob)\n","#         self.classifier = model.classifier\n","#         self.loss_fct = nn.MSELoss()\n","\n","#     def forward(self,original_model_output,inputs):\n","#         sequence_output = self.dropout(original_model_output)\n","#         logits = self.classifier(sequence_output)\n","#         loss = loss_fct(logits,inputs['labels'])\n","#         return {'loss':loss,'preds':logits,'labels':inputs['labels']}\n","\n","# class TaggingHead(nn.Module):\n","\n","#     def __init__(self,task_name,mini_task_dict,pretrained_model_name='distilroberta-base',shared_encoder_layer=None):\n","#         super().__init__()\n","#         self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","#         config = AutoConfig.from_pretrained(pretrained_model_name,\n","#                                                         num_labels=mini_task_dict['n_labels'],\n","#                                                         output_hidden_states=True)\n","#         self.config = config\n","#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","#         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","#         self.class_weights = mini_task_dict['class_weights']\n","#         self.loss_fct = nn.CrossEntropyLoss(weight = self.class_weights) if self.class_weights else nn.CrossEntropyLoss()\n","#         self.shared_encoder_layer = shared_encoder_layer\n","\n","#     def set_shared_encoder_layer(self,encoder_layer):\n","#         self.shared_encoder_layer = encoder_layer\n","\n","#     def forward(self,original_model_output,inputs):\n","#         output = self.shared_encoder_layer(original_model_output).last_hidden_state if self.shared_encoder_layer else original_model_output\n","#         sequence_output = self.dropout(output)\n","#         logits = self.classifier(sequence_output)\n","#         loss = None\n","#         labels = inputs['labels']\n","#         # Only keep active parts of the loss\n","#         active_loss = inputs['attention_mask'].view(-1) == 1\n","#         active_logits = logits.view(-1, self.config.num_labels)\n","#         active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(self.loss_fct.ignore_index).type_as(labels))\n","#         non_padded_mask = active_labels!=-100\n","#         loss = self.loss_fct(active_logits[non_padded_mask], active_labels[non_padded_mask])\n","#         active_preds_mask = torch.logical_and(active_loss,non_padded_mask)\n","#         return {'loss':loss,'preds':torch.argmax(logits,2).flatten()[active_preds_mask],'labels':active_labels[non_padded_mask]}\n","\n","# class MultiTaskModel(nn.Module):\n","\n","#     def __init__(self,pretrained_model='distilroberta-base',kwargs_dict=None,current_task=None):\n","#         super().__init__()\n","#         self.kwargs_dict = kwargs_dict\n","#         self.primary_task = self.kwargs_dict['task_priorities_priority_as_key']['primary_task']\n","#         self.model = self.kwargs_dict[self.primary_task]['model'].from_pretrained(pretrained_model,\n","#                                                                num_labels=self.kwargs_dict[self.primary_task]['n_labels'],\n","#                                                                output_hidden_states=True)\n","#         self.shared_encoder_layer = None\n","#         self.current_task = current_task\n","#         self.decoder_dict = {}\n","#         if self.kwargs_dict['shared_encoder_n_layers']>0 and self.shared_encoder_layer==None:\n","#             config = AutoConfig.from_pretrained(pretrained_model,num_hidden_layers=self.kwargs_dict['shared_encoder_n_layers'],output_hidden_states=True)\n","#             shared_encoder = AutoModel.from_pretrained(pretrained_model,config=config).encoder\n","#             self.shared_encoder_layer = shared_encoder\n","#         for task in self.kwargs_dict['tasks']:\n","#             if task in self.kwargs_dict['classification_tasks']:\n","#                 self.decoder_dict[task] = TaggingHead(task,self.kwargs_dict[task])\n","#                 if self.kwargs_dict[task]['shares_encoder'] and self.shared_encoder_layer:\n","#                     self.decoder_dict[task].set_shared_encoder_layer(self.shared_encoder_layer)\n","#             elif task in self.kwargs_dict['regression_tasks'] and task != self.primary_task :\n","#                 self.decoder_dict[task] = ClassificationHead(task_name,self.kwargs_dict[task])\n","\n","#     def forward(self,**inputs):\n","#         dataset = torch.unique(inputs['dataset'])[0] if len(torch.unique(inputs['dataset']))==1 else 'mix'\n","#         if dataset!='mix':\n","#             outputs_dict = self.get_outputs(inputs,dataset)\n","#         else:\n","#             outputs_dict = {}\n","#             for task in self.split_inputs(inputs):\n","#                 outputs = self.get_outputs(self,inputs=task[1],dataset=0) if task[0]=='fce_task' else self.get_outputs(self,inputs=task[1],dataset=1) \n","#                 outputs_dict = {**outputs_dict,**outputs}\n","#         return outputs_dict\n","\n","\n","#     def get_outputs(self,inputs,dataset):\n","#         if dataset==0:\n","#             if self.primary_task in self.kwargs_dict['regression_tasks']:\n","#                 model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'],labels=inputs[self.kwargs_dict['labels'][self.primary_task]].float())\n","#                 outputs_dict = {\n","#                                 f'{self.primary_task}_loss':model_output.loss,\n","#                                 f'{self.primary_task}_preds':model_output.logits.flatten(),\n","#                                 f'{self.primary_task}_labels':inputs[self.kwargs_dict['labels'][self.primary_task]]\n","#                                 }\n","#             else:\n","#                 model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n","#                 primary_output = self.decoder_dict[self.primary_task](model_output.hidden_states[self.kwargs_dict[self.primary_task]['output_layer']])\n","#                 outputs_dict = {}\n","\n","#             tasks = list(set(self.kwargs_dict['tasks'])-{'ner'}) if self.primary_task not in self.kwargs_dict['regression_tasks'] else ['ged']\n","#             for task in tasks:\n","#                 output = self.decoder_dict[task](model_output.hidden_states[self.kwargs_dict[task]['output_layer']])\n","#                 outputs_dict = self.get_output_dict(task,output,outputs_dict)\n","#         else:\n","#             model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n","#             task='ner'\n","#             output = self.decoder_dict[task](model_output.hidden_states[self.kwargs_dict[task]['output_layer']])\n","#             outputs_dict = self.get_output_dict(task,output,outputs_dict={})\n","#         return outputs_dict\n","\n","#     def get_output_dict(self,task,output,output_dict):\n","#         return {**outputs_dict,**{f'{task}_loss':output['loss'],f'{task}_preds':output['preds'],f'{task}_labels':inputs[self.kwargs_dict['labels'][task]]}}\n","\n","#     def split_inputs(self,inputs):\n","#         split_mask_1 = inputs['dataset']=='fce'\n","#         return ((['fce_task'],{k:v[split_mask_1] for k,v in inputs.items()}),('ner',{k:v[~split_mask_1] for k,v in inputs.items()}))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"adhGhXiNwSvL","executionInfo":{"status":"error","timestamp":1630453161213,"user_tz":-60,"elapsed":88414,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"09e23b43-9591-4966-ba19-b3049eaf0e2f"},"source":["from transformers import TrainingArguments,Trainer,AutoModelForSequenceClassification,AutoModel\n","import numpy as np\n","from math import floor\n","from torch import tensor\n","from torch.cuda import is_available\n","from collections import defaultdict\n","from scipy.stats import spearmanr\n","from sklearn.metrics import cohen_kappa_score,classification_report,fbeta_score\n","\n","class ClassificationHead(nn.Module):\n","\n","    def __init__(self,task_name,mini_task_dict,pretrained_model_name='distilroberta-base',shared_encoder_layer=None):\n","        super().__init__()\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        model = mini_task_dict['model'].from_pretrained(pretrained_model_name,\n","                                                        num_labels=mini_task_dict['n_labels'],\n","                                                        output_hidden_states=True)\n","        self.dropout = nn.Dropout(model.config.hidden_dropout_prob)\n","        self.classifier = model.classifier\n","        self.loss_fct = nn.MSELoss()\n","\n","    def forward(self,original_model_output,inputs):\n","        sequence_output = self.dropout(original_model_output)\n","        logits = self.classifier(sequence_output)\n","        loss = loss_fct(logits,inputs['labels'])\n","        return {'loss':loss,'preds':logits,'labels':inputs['labels']}\n","\n","class TaggingHead(nn.Module):\n","\n","    def __init__(self,task_name,mini_task_dict,pretrained_model_name='distilroberta-base',shared_encoder_layer=None):\n","        super().__init__()\n","        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","        config = AutoConfig.from_pretrained(pretrained_model_name,\n","                                                        num_labels=mini_task_dict['n_labels'],\n","                                                        output_hidden_states=True)\n","        self.config = config\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(self.device)\n","        self.class_weights = mini_task_dict['class_weights']\n","        self.loss_fct = nn.CrossEntropyLoss(weight = self.class_weights) if self.class_weights != None else nn.CrossEntropyLoss()\n","        self.shared_encoder_layer = shared_encoder_layer\n","\n","    def set_shared_encoder_layer(self,encoder_layer):\n","        self.shared_encoder_layer = encoder_layer\n","\n","    def forward(self,original_model_output,inputs):\n","        output = self.shared_encoder_layer(original_model_output).last_hidden_state if self.shared_encoder_layer else original_model_output\n","        sequence_output = self.dropout(output)\n","        logits = self.classifier(sequence_output)\n","        loss = None\n","        labels = inputs['labels']\n","        # Only keep active parts of the loss\n","        active_loss = inputs['attention_mask'].view(-1) == 1\n","        active_logits = logits.view(-1, self.config.num_labels)\n","        active_labels = torch.where(active_loss, labels.view(-1), torch.tensor(self.loss_fct.ignore_index).type_as(labels))\n","        non_padded_mask = active_labels!=-100\n","        loss = self.loss_fct(active_logits[non_padded_mask], active_labels[non_padded_mask])\n","        active_preds_mask = torch.logical_and(active_loss,non_padded_mask)\n","        return {'loss':loss,'preds':torch.argmax(logits,2).flatten()[active_preds_mask],'labels':active_labels[non_padded_mask]}\n","\n","class MultiTaskModel(nn.Module):\n","\n","    def __init__(self,pretrained_model='distilroberta-base',kwargs_dict=None,current_task=None):\n","        super().__init__()\n","        self.kwargs_dict = kwargs_dict\n","        self.primary_task = self.kwargs_dict['task_priorities_priority_as_key']['primary_task']\n","        self.model = self.kwargs_dict[self.primary_task]['model'].from_pretrained(pretrained_model,\n","                                                               num_labels=self.kwargs_dict[self.primary_task]['n_labels'],\n","                                                               output_hidden_states=True)\n","        self.shared_encoder_layer = None\n","        self.current_task = current_task\n","        self.decoder_dict = {}\n","        if self.kwargs_dict['shared_encoder_n_layers']>0 and self.shared_encoder_layer==None:\n","            config = AutoConfig.from_pretrained(pretrained_model,num_hidden_layers=self.kwargs_dict['shared_encoder_n_layers'],output_hidden_states=True)\n","            shared_encoder = AutoModel.from_pretrained(pretrained_model,config=config).encoder\n","            self.shared_encoder_layer = shared_encoder\n","        for task in self.kwargs_dict['tasks']:\n","            if task in self.kwargs_dict['classification_tasks']:\n","                self.decoder_dict[task] = TaggingHead(task,self.kwargs_dict[task])\n","                if self.kwargs_dict[task]['shares_encoder'] and self.shared_encoder_layer:\n","                    self.decoder_dict[task].set_shared_encoder_layer(self.shared_encoder_layer)\n","            elif task in self.kwargs_dict['regression_tasks'] and task != self.primary_task :\n","                self.decoder_dict[task] = ClassificationHead(task_name,self.kwargs_dict[task])\n","\n","    def forward(self,**inputs):\n","        dataset = torch.unique(inputs['dataset'])[0] if len(torch.unique(inputs['dataset']))==1 else 'mix'\n","        if dataset!='mix':\n","            outputs_dict = self.get_outputs(inputs,dataset)\n","        else:\n","            outputs_dict = {}\n","            for task in self.split_inputs(inputs):\n","                outputs = self.get_outputs(inputs=task[1],dataset=0) if task[0]=='fce_task' else self.get_outputs(inputs=task[1],dataset=1) \n","                outputs_dict = {**outputs_dict,**outputs}\n","        return outputs_dict\n","\n","\n","    def get_outputs(self,inputs,dataset):\n","        if dataset==0:\n","            if self.primary_task in self.kwargs_dict['regression_tasks']:\n","\n","                model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'],labels=inputs[self.kwargs_dict[self.primary_task]['labels']].float())\n","                outputs_dict = {\n","                                f'{self.primary_task}_loss':model_output.loss,\n","                                f'{self.primary_task}_preds':model_output.logits.flatten(),\n","                                f'{self.primary_task}_labels':inputs[self.kwargs_dict[self.primary_task]['labels']]\n","                                }\n","            else:\n","                model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n","                # primary_output = self.decoder_dict[self.primary_task](model_output.hidden_states[self.kwargs_dict[self.primary_task]['output_layer']])\n","                outputs_dict = {}\n","\n","            tasks = list(set(self.kwargs_dict['tasks'])-{'ner'}) if self.primary_task not in self.kwargs_dict['regression_tasks'] else ['ged']\n","            for task in tasks:\n","                output = self.decoder_dict[task](model_output.hidden_states[self.kwargs_dict[task]['output_layer']],inputs)\n","                outputs_dict = self.get_output_dict(task,output,outputs_dict)\n","        else:\n","            model_output = self.model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])\n","            task='ner'\n","            output = self.decoder_dict[task](model_output.hidden_states[self.kwargs_dict[task]['output_layer']],inputs)\n","            outputs_dict = self.get_output_dict(task,output,outputs_dict={})\n","        return outputs_dict\n","\n","    def get_output_dict(self,task,output,outputs_dict):\n","        return {**outputs_dict,**{f'{task}_loss':output['loss'],f'{task}_preds':output['preds'],f'{task}_labels':output[self.kwargs_dict[task]['labels']]}}\n","\n","    def split_inputs(self,inputs):\n","        split_mask = inputs['dataset']==0\n","        return ((['fce_task'],{k:v[split_mask] for k,v in inputs.items()}),('ner',{k:v[~split_mask] for k,v in inputs.items()}))\n","\n","def training_args(batch_size=8,save_strategy='no',output_dir='/',lr= 5e-4,epochs=30,weight_decay=0.01):\n","\targs = TrainingArguments(\n","\t    output_dir=output_dir,\n","\t    save_strategy=save_strategy,\n","\t    evaluation_strategy='no',\n","\t    learning_rate=lr,\n","\t    per_device_train_batch_size=batch_size,\n","\t    per_device_eval_batch_size=batch_size,\n","\t    num_train_epochs=epochs,\n","\t    weight_decay=weight_decay,\n","\t)\n","\treturn args\n","\n","def training_kwargs(\n","    tasks=['aes','ged','ner'],outputs=['loss','labels','preds'],optimizer_weighting='fixed',\n","    init_task_weightings={'aes':1.0,'ged':1.0,'ner':1.0},task_priorities={'aes':'primary_task','ged':'secondary_task','ner':'aux_task'},\n","    temp = 2,metrics_to_track_by_task={'aes':'pearson_aes','ged':'f_0_5_ged','ner':'f1_score_avg_ner'},\n","    regression_tasks=['aes'],classification_tasks=['ner','ged'],fce_tasks=['aes','ged'], \n","    class_weights={'aes':None,'ged':None,'ner':None},n_labels={'aes':1,'ged':2,'ner':9},labels={'aes':'scores','ged':'labels','ner':'labels'},\n","    model={'aes':AutoModelForSequenceClassification,'ged':AutoModel,'ner':AutoModel},\n","    shares_encoder={'aes':False,'ged':True,'ner':True},output_layer={'aes':-1,'ged':-1,'ner':-1},shared_encoder_n_layers = 1,trainer=True,\n","    early_stopping_patience=3,early_stopping_metric=None,pretrained_model='distilroberta-base',frozen_layers='all',scoring='script'\n","    ):\n","    if type(init_task_weightings)==int:\n","        init_task_weightings = calculate_init_loss_hyper_params(init_task_weightings,tasks,task_priorities)\n","\n","    map_task_index = {task:i for i,task in enumerate(tasks)}\n","    map_out_to_task={f'{task}_{output}':task  for task in tasks for output in outputs}\n","    map_out_to_out={f'{task}_{output}':output  for task in tasks for output in outputs}\n","    task_priorities_priority_as_key = {v:k for k,v in task_priorities.items()}\n","    kwargs = {\n","        'tasks':tasks,\n","        'outputs':outputs,\n","        'map_out_to_task':map_out_to_task,\n","        'map_out_to_out':map_out_to_out,\n","        'map_task_index':map_task_index,\n","        'map_index_task':{v:k for k,v in map_task_index.items()},\n","        'optimizer_weighting':optimizer_weighting,\n","        'epochs_to_avg_over':2,\n","        'temp':temp,\n","        'init_task_weightings':init_task_weightings,\n","        'metrics_to_track_by_task':metrics_to_track_by_task,\n","        'regression_tasks':regression_tasks,\n","        'classification_tasks':classification_tasks,\n","        'fce_tasks':fce_tasks,\n","        'task_priorities_priority_as_key':task_priorities_priority_as_key,\n","        'early_stopping_patience':early_stopping_patience,\n","        'early_stopping_metric':early_stopping_metric,\n","        'pretrained_model':pretrained_model,\n","        'frozen_layers':frozen_layers,\n","        'scoring':scoring\n","    }\n","    if trainer:\n","        return kwargs\n","    else:\n","        task_dict = {\n","            task:{\n","              'class_weights':class_weights[task],\n","              'n_labels':n_labels[task],\n","              'labels':labels[task],\n","              'model':model[task],\n","              'shares_encoder':shares_encoder[task],\n","              'output_layer':output_layer[task]\n","              } \n","            for task in tasks\n","            }\n","        task_dict['shared_encoder']=True\n","        task_dict['shared_encoder_n_layers']=shared_encoder_n_layers\n","        task_dict['fce_tasks'] = fce_tasks\n","        return {**task_dict,**kwargs}\n","\n","def calculate_init_loss_hyper_params(primary_task_weight,tasks,task_priorities):\n","    updated_init_task_weightings = {}\n","    for task in tasks:\n","        if task_priorities[task]=='primary_task':\n","            updated_init_task_weightings[task] = primary_task_weight\n","        elif task_priorities[task]=='secondary_task':\n","            updated_init_task_weightings[task] = 1-primary_task_weight\n","        elif task_priorities[task]=='aux_task':\n","            updated_init_task_weightings[task] = (1-primary_task_weight)*0.1\n","    return updated_init_task_weightings\n","\n","class MultiTaskModelTrainer(Trainer):\n","\n","    prev_epoch = 0\n","    device = 'cuda' if is_available() else 'cpu'\n","\n","    def compute_loss(self,model,inputs,return_outputs=False):\n","    \t# round down epoch \n","        epoch = floor(self.state.epoch)\n","        # calculate the current step within an epoch\n","        step_in_epoch = self.state.global_step+1-(epoch*(self.state.max_steps/self.state.num_train_epochs))\n","        if step_in_epoch==1:\n","            self.model.train()   \n","            # inits for start of training\n","            if self.state.global_step == 0:\n","                # avg cost across predetermined number of epochs (init as zero for first epoch) used for dwa\n","                self.avg_cost = torch.zeros([self.state.num_train_epochs, len(self.kwargs_['tasks'])]).float().to(self.device)\n","                # a tensoer where each col represetnts the task and each row represents its value at a given epoch\n","                indexes , weights = zip(*sorted([(self.kwargs_['map_task_index'][k],v) for k,v in self.kwargs_['init_task_weightings'].items()]))\n","                self.lambda_weights = tensor(list(weights)).expand(self.state.num_train_epochs,-1).to(self.device)\n","\n","        if self.kwargs_['optimizer_weighting']=='dwa':\n","            # if the first step of any epoch update task weighing values \n","            if step_in_epoch==1:\n","                if epoch not in list(range(self.kwargs_[epochs_to_avg_over])):\n","                    # find the change in loss over the past number of spesified epochs for each task\n","                    ws = [torch.exp((self.avg_cost[epoch - 1, val] / self.avg_cost[epoch - self.kwargs_[epochs_to_avg_over], val])/temp) for val in range(len(self.kwargs_['tasks']))]\n","                    # updates weights in accordance with change in loss\n","                    self.lambda_weights[epoch, :] = tensor([len(self.kwargs_['tasks']) * (ws[val]/sum(ws)) for val in range(len(self.kwargs_['tasks']))])\n","\n","        if self.kwargs_['optimizer_weighting']=='dynamic':\n","            if step_in_epoch==1:\n","                self.update_te(epoch)\n","                self.lambda_weights[epoch, :] = tensor([self.calc_dynamic_loss(task,epoch) for task in self.kwargs_['tasks']])\n","\n","\n","        outputs = self.model(**inputs)\n","        # get training loss for current task based off of current mini batch\n","        loss =  self.sum_losses(outputs,epoch)\n","\n","        # if final step in epoch perform evaluation\n","        if (self.state.global_step+1)%(self.state.max_steps/self.state.num_train_epochs)==0:\n","            self.compute_metrics_2(eval_dataset=self.eval_dataset,epoch=floor(self.state.epoch),testing=False)\n","            self.prev_epoch = self.state.epoch\n","\n","        return loss\n","\n","    def sum_losses(self,outputs,epoch,eval=False):\n","        train_loss = {}\n","        for task in self.kwargs_['tasks']:\n","            try:\n","                output_loss = outputs[f'{task}_loss']\n","                train_loss[task] = output_loss\n","            except:\n","                continue\n","        \n","        losses = {}\n","        for k,v in train_loss.items():\n","            losses[k] = self.lambda_weights[epoch, self.kwargs_['map_task_index'][k]] * v\n","            if self.kwargs_['optimizer_weighting']=='dwa':\n","                self.avg_cost[epoch, k] += train_loss[self.kwargs_['map_task_index'][k]].item() / (self.state.max_steps/self.state.num_train_epochs)\n","        if eval:\n","            return losses\n","        else:\n","            return sum(losses.values())\n","\n","\n","    def compute_metrics_2(self,eval_dataset,epoch,testing=False):\n","        \n","        self.model.eval()\n","        history = {task:defaultdict(list) for task in self.kwargs_['tasks']}\n","        losses = defaultdict(int)\n","        total_loss = 0.0\n","        # add all outputs to device\n","        for step,inputs in enumerate(self.get_eval_dataloader(eval_dataset)):     \n","            for key, value in inputs.items():\n","                inputs[key] = inputs[key].to(self.device)\n","            # get output from model using mini batch\n","            outputs = self.model(**inputs)\n","            calculated_loses = self.sum_losses(outputs,epoch,eval=True)\n","\n","            for task,value in calculated_loses.items():\n","                # add loss to the total loss of the model and inidividual task losses\n","                losses[task] += value.item()\n","                total_loss += value.item()\n","            for key,value in outputs.items():\n","                # appnd the list of predictions based off the task and output\n","                if 'loss' not in key and 'dataset' != key:\n","                    history[self.kwargs_['map_out_to_task'][key]][self.kwargs_['map_out_to_out'][key]] = history[self.kwargs_['map_out_to_task'][key]][self.kwargs_['map_out_to_out'][key]] + value.cpu().detach().numpy().tolist()\n","\n","        if self.kwargs_['optimizer_weighting'] in['dwa','dynamic']:\n","            for task in tasks:\n","                print( f'{task}_weight_coef : {self.lambda_weights[self.kwargs_[\"map_task_index\"][task] , epoch]}')\n","            print()\n","        avg_losses = {k:v/step for k,v in losses.items()}\n","        avg_losses['total_loss'] = total_loss/step\n","        print('losses',avg_losses)\n","        print()\n","\n","        metrics_dics = []\n","        for task,values in history.items():\n","            if task in self.kwargs_['regression_tasks']:\n","            \t  metrics_dics.append(self.calc_regression_metrics(np.rint(values['preds']),values['labels'],task))\n","            elif task in self.kwargs_['classification_tasks']:\n","                metrics_dics.append(self.calc_classification_metrics(values['preds'],values['labels'],task))\n","        \n","            logs = {k:v for dic in metrics_dics for k,v in dic.items()}\n","            logs = {**logs,**avg_losses}\n","        if testing==False:\n","            self.log(logs)\n","            log_hist = [epoch for epoch in self.state.log_history for k,v in epoch.items() if 'learning_rate' not in epoch.keys()]\n","            self.best_metrics_values = {task : max([epoch[metric] for epoch in log_hist]) for task,metric in self.kwargs_['metrics_to_track_by_task'].items() }\n","            self.best_metrics_epoch = {task : np.argmax([epoch[metric] for epoch in log_hist]) for task,metric in self.kwargs_['metrics_to_track_by_task'].items() }\n","            self.lowest_loss_epoch = np.argmin([epoch[f'{self.kwargs_[\"task_priorities_priority_as_key\"][\"primary_task\"]}'] for epoch in log_hist])\n","            if self.kwargs_['early_stopping_patience']:\n","                if self.kwargs_['early_stopping_metric'] == 'loss':\n","                    if (floor(self.state.epoch) - self.lowest_loss_epoch) == self.kwargs_['early_stopping_patience']:\n","                        self.save_hist_and_stop_training(log_hist)\n","\n","                else:\n","                    if (floor(self.state.epoch) - self.best_metrics_epoch[self.kwargs_['task_priorities_priority_as_key']['primary_task']]) == self.kwargs_['early_stopping_patience']:\n","                        self.save_hist_and_stop_training(log_hist)\n","        else:\n","            return logs,{task:{'preds':values['preds'],'labels':values['labels']} for task,values in history.items() if task in self.kwargs_['regression_tasks']}\n","    \n","    def save_hist_and_stop_training(self,log_hist):\n","        best_hist = {task : {metric:value for metric,value in log_hist[epoch].items() if task in metric} for task,epoch in self.best_metrics_epoch.items()}\n","        eval_hist = defaultdict(list)\n","        [eval_hist[k].append(v) for epoch in log_hist for k,v in epoch.items()]\n","        try:\n","            del eval_hist['total_flos']\n","        except:\n","            pass\n","        try:\n","            del eval_hist['total_flos']\n","            train_hist =  {}\n","            train_hist['train_loss'] = eval_hist.pop('train_loss')\n","            train_hist['train_runtime'] = eval_hist.pop('train_runtime')\n","            train_hist['train_samples_per_second'] = eval_hist.pop('train_samples_per_second')\n","            train_hist['train_steps_per_second'] = eval_hist.pop('train_steps_per_second')\n","            train_hist['train_samples_per_second'] = eval_hist.pop('train_samples_per_second')\n","        except:\n","            train_hist = {}\n","        test_hist,test_preds = self.compute_metrics_2(eval_dataset=self.test_dataset,epoch=self.best_metrics_epoch[self.kwargs_[\"task_priorities_priority_as_key\"][\"primary_task\"]],testing=True)\n","        info_dict = self.kwargs_\n","        hist = {}\n","        hist['best'],hist['train'],hist['eval'],hist['test'],hist['info'],hist['preds'] = best_hist,train_hist,eval_hist,test_hist,info_dict,test_preds\n","        self.history_dict = hist\n","        with open(f'results/{time.time()}.json', 'w') as fp:\n","            json.dump(hist, fp)\n","        self.state.max_steps = self.state.global_step\n","\n","\n","    def calc_regression_metrics(self,preds,labels,task):\n","        metrics_dic = {\n","            f\"rmse_{task}\": np.sqrt(np.mean((preds-labels)**2)),\n","            f\"pearson_{task}\": np.corrcoef(preds,labels)[0,1],\n","            f\"spearman_{task}\" : spearmanr(preds, labels)[0],\n","            f\"kappa_{task}\":cohen_kappa_score(preds,labels,weights='quadratic')\n","            }\n","        print(f'{task}_metrics',metrics_dic)\n","        print()\n","        return metrics_dic\n","\n","    def calc_classification_metrics(self,preds,labels,task):\n","        digits = 9 if task == 'ner' else 2\n","        print(task,classification_report(labels, preds, digits=digits))\n","        report_output = classification_report(labels, preds, digits=digits, output_dict=True)\n","        metrics_dic = {\n","          f'f1_score_avg_{task}' : report_output['accuracy'],\n","          f'f1_score_macro_{task}' : report_output['macro avg']['f1-score'],\n","          f'f1_score_weighted_{task}' : report_output['weighted avg']['f1-score'],\n","    \t  }\n","        if digits==2:\n","            metrics_dic[f'f_0_5_{task}'] = fbeta_score(preds,labels,beta=0.5)\n","        print(f'{task}_metrics',metrics_dic )\n","        print()\n","        return metrics_dic\n","\n","    def calc_dynamic_loss(self,task,epoch):\n","        if task in self.kwargs_['regression_tasks']:\n","            return self.te\n","        elif task in self.kwargs_['classification_tasks']:\n","            return (1-self.te)\n","\n","    def update_te(self,epoch):\n","        if epoch==0:\n","            self.gamma = np.log((1/1e-6)-1)/((self.args.num_train_epochs/2)-1)\n","            self.te = 1/(1+np.exp(self.gamma*((self.args.num_train_epochs/2)-epoch)))\n","\n","    def get_train_dataloader(self):\n","        train_dataset = self.train_dataset\n","        return torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=self.args.train_batch_size,\n","            collate_fn=self.data_collator,\n","            drop_last=self.args.dataloader_drop_last,\n","            num_workers=self.args.dataloader_num_workers,\n","            pin_memory=self.args.dataloader_pin_memory,\n","        )\n","\n","    def get_eval_dataloader(self,eval_dataset):\n","        return torch.utils.data.DataLoader(\n","            eval_dataset,\n","            batch_size=self.args.train_batch_size,\n","            collate_fn=self.data_collator,\n","            drop_last=self.args.dataloader_drop_last,\n","            num_workers=self.args.dataloader_num_workers,\n","            pin_memory=self.args.dataloader_pin_memory,\n","        )\n","\n","model = MultiTaskModel(kwargs_dict = training_kwargs(class_weights={'aes':None,'ged':dataset_obj.get_weights(),'ner':None},trainer=False))\n","for name,params in model.model.named_parameters():\n","    if 'classifier' not in name:\n","        params.requires_grad = False\n","args = training_args()\n","trainer = MultiTaskModelTrainer(\n","    model,\n","    args,\n","    train_dataset=dataset_dict['train'],\n","    eval_dataset=dataset_dict['dev'],\n",")\n","trainer.kwargs_ = training_kwargs(trainer=True)\n","trainer.test_dataset = dataset_dict['test']\n","trainer.train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n","Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 1,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1\n","Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'lm_head.decoder.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.3.output.dense.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'lm_head.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of RobertaModel were initialized from the model checkpoint at distilroberta-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n","loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\",\n","    \"4\": \"LABEL_4\",\n","    \"5\": \"LABEL_5\",\n","    \"6\": \"LABEL_6\",\n","    \"7\": \"LABEL_7\",\n","    \"8\": \"LABEL_8\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3,\n","    \"LABEL_4\": 4,\n","    \"LABEL_5\": 5,\n","    \"LABEL_6\": 6,\n","    \"LABEL_7\": 7,\n","    \"LABEL_8\": 8\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 6,\n","  \"output_hidden_states\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.10.0\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","***** Running training *****\n","  Num examples = 2114\n","  Num Epochs = 30\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 7950\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='374' max='7950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 374/7950 01:23 < 28:15, 4.47 it/s, Epoch 1.41/30]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","text":["losses {'aes': 7.006794596827308, 'ged': 0.13928976862929587, 'ner': 0.16011982116588327, 'total_loss': 7.3062041866224865}\n","\n","aes_metrics {'rmse_aes': 5.779513628133234, 'pearson_aes': 0.37877302864959617, 'spearman_aes': 0.39060947837014537, 'kappa_aes': 0.06899128268991284}\n","\n","ged               precision    recall  f1-score   support\n","\n","           0       0.89      0.87      0.88     25174\n","           1       0.28      0.33      0.30      3917\n","\n","    accuracy                           0.80     29091\n","   macro avg       0.59      0.60      0.59     29091\n","weighted avg       0.81      0.80      0.80     29091\n","\n","ged_metrics {'f1_score_avg_ged': 0.7986318792753773, 'f1_score_macro_ged': 0.5934380378238653, 'f1_score_weighted_ged': 0.8044897380094166, 'f_0_5_ged': 0.3179677819083023}\n","\n","ner               precision    recall  f1-score   support\n","\n","           0  0.960108135 0.988400103 0.974048722     42759\n","           1  0.916269571 0.730727470 0.813047418      1842\n","           2  0.985248447 0.970925784 0.978034682      1307\n","           3  0.710823910 0.656226696 0.682435052      1341\n","           4  0.762284197 0.764314248 0.763297872       751\n","           5  0.670206819 0.652694611 0.661334804      1837\n","           6  0.666666667 0.708171206 0.686792453       257\n","           7  0.667763158 0.220173536 0.331158238       922\n","           8  0.707423581 0.468208092 0.563478261       346\n","\n","    accuracy                      0.936061680     51362\n","   macro avg  0.782977165 0.684426861 0.717069723     51362\n","weighted avg  0.930987730 0.936061680 0.930752797     51362\n","\n","ner_metrics {'f1_score_avg_ner': 0.9360616798411276, 'f1_score_macro_ner': 0.7170697225063046, 'f1_score_weighted_ner': 0.9307527969094216}\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-4e2e4b4b95c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1282\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1787\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1789\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-4e2e4b4b95c7>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;31m# get training loss for current task based off of current mini batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-4e2e4b4b95c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **inputs)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'mix'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'mix'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0moutputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moutputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-4e2e4b4b95c7>\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(self, inputs, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mtasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tasks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_task\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'regression_tasks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'ged'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0moutputs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-4e2e4b4b95c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, original_model_output, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mactive_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mnon_padded_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_padded_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_padded_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"c8txkcvyc1a0"},"source":["log_hist = [epoch for epoch in trainer.state.log_history for k,v in epoch.items() if 'learning_rate' not in epoch.keys()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z-w-eFCpwl6q","executionInfo":{"status":"ok","timestamp":1630453193359,"user_tz":-60,"elapsed":7879,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"007cab32-1d3a-4a8c-d4ec-1fb31fc1788c"},"source":["trainer.save_hist_and_stop_training(log_hist)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["losses {'aes': 9.145910399300712, 'ged': 0.19631987526303246, 'ner': 0.15572395778837658, 'total_loss': 9.49795423235212}\n","\n","aes_metrics {'rmse_aes': 5.62175832517431, 'pearson_aes': 0.6513049882002766, 'spearman_aes': 0.6600941274408868, 'kappa_aes': 0.17117951091568306}\n","\n","ged               precision    recall  f1-score   support\n","\n","           0       0.88      0.92      0.90     31755\n","           1       0.38      0.28      0.32      5670\n","\n","    accuracy                           0.82     37425\n","   macro avg       0.63      0.60      0.61     37425\n","weighted avg       0.80      0.82      0.81     37425\n","\n","ged_metrics {'f1_score_avg_ged': 0.821376085504342, 'f1_score_macro_ged': 0.6083254454882531, 'f1_score_weighted_ged': 0.809666868023565, 'f_0_5_ged': 0.29236388029664967}\n","\n","ner               precision    recall  f1-score   support\n","\n","           0  0.961643416 0.983926102 0.972657157     38323\n","           1  0.882768362 0.773036487 0.824266403      1617\n","           2  0.959697733 0.988754325 0.974009374      1156\n","           3  0.746397695 0.623720650 0.679567071      1661\n","           4  0.822510823 0.682634731 0.746073298       835\n","           5  0.836295603 0.535971223 0.653270004      1668\n","           6  0.684466019 0.548638132 0.609071274       257\n","           7  0.429139073 0.461538462 0.444749485       702\n","           8  0.318181818 0.745370370 0.445983380       216\n","\n","    accuracy                      0.930892646     46435\n","   macro avg  0.737900060 0.704843387 0.705516383     46435\n","weighted avg  0.931566760 0.930892646 0.929049169     46435\n","\n","ner_metrics {'f1_score_avg_ner': 0.9308926456336815, 'f1_score_macro_ner': 0.7055163828585633, 'f1_score_weighted_ner': 0.9290491692346714}\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OzbN4EoN1IcB","executionInfo":{"status":"ok","timestamp":1630451579821,"user_tz":-60,"elapsed":207,"user":{"displayName":"Cameron Stronge","photoUrl":"","userId":"13361440774782721572"}},"outputId":"ab4dee65-0c37-406f-8ac2-baee7abcea78"},"source":["best_scores,train_hist,eval_hist,test_hist,info_dict\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'aes': 7.0047174054522845,\n","  'epoch': 1.0,\n","  'f1_score_avg_ged': 0.748925784606923,\n","  'f1_score_avg_ner': 0.9352828939683034,\n","  'f1_score_macro_ged': 0.5825031745614321,\n","  'f1_score_macro_ner': 0.6992174443121277,\n","  'f1_score_weighted_ged': 0.7751119523551384,\n","  'f1_score_weighted_ner': 0.9280180117469411,\n","  'f_0_5_ged': 0.38042269187986644,\n","  'ged': 0.13772699028946633,\n","  'kappa_aes': 0.06308401511690687,\n","  'ner': 0.16539328181466392,\n","  'pearson_aes': 0.3380663611717876,\n","  'rmse_aes': 5.792715732327589,\n","  'spearman_aes': 0.33919986498837235,\n","  'step': 264,\n","  'total_loss': 7.307837677556415},\n"," {'epoch': 1.89,\n","  'learning_rate': 0.0004685534591194969,\n","  'loss': 37.5006,\n","  'step': 500},\n"," {'aes': 5.951165908990904,\n","  'epoch': 2.0,\n","  'f1_score_avg_ged': 0.8358942628304287,\n","  'f1_score_avg_ner': 0.941785756006386,\n","  'f1_score_macro_ged': 0.6191960677203276,\n","  'f1_score_macro_ner': 0.7672190406167556,\n","  'f1_score_weighted_ged': 0.8291005349545686,\n","  'f1_score_weighted_ner': 0.9399376925319006,\n","  'f_0_5_ged': 0.3138064243001535,\n","  'ged': 0.14174374175626178,\n","  'kappa_aes': 0.23060762766645126,\n","  'ner': 0.1421637859108836,\n","  'pearson_aes': 0.5464335135229957,\n","  'rmse_aes': 5.376130372096437,\n","  'spearman_aes': 0.5582998033036058,\n","  'step': 529,\n","  'total_loss': 6.23507343665805}]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"hO7hX6B71iIL"},"source":["self.history_dict = hist\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9RHHciBi1kGT"},"source":["args = training_args()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trug6RXo3sOn"},"source":["args"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vak_Seac3vBT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-PDT0NB5EE6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uyce0CYc4LC-"},"source":[""],"execution_count":null,"outputs":[]}]}